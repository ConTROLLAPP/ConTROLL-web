import os
import re
import time
import random
import requests
from typing import Dict, Set, Any
from api_usage_tracker import check_api_quota
import json
from shared_guest_alerts import add_shared_guest_profile, check_shared_guest_alert, get_shared_guest_count

# Load secrets from secrets.json
try:
    with open("secrets.json") as f:
        secrets = json.load(f)
except FileNotFoundError:
    print("‚ö†Ô∏è secrets.json not found, falling back to environment variables")
    secrets = {
        "SERPER_API_KEY": os.environ.get('SERPER_API_KEY', '1d67ed1df4aee6acf1491b1bbcbdf82b545473cf'),
        "PUPPETEER_ENDPOINT": "https://controll-puppeteer.onrender.com",
        "GITHUB_TOKEN": os.environ.get('GITHUB_TOKEN', '')
    }

def normalize_alias(alias):
    """
    Normalize aliases by removing punctuation and standardizing format.
    This ensures 'Seth D.', 'Seth D', 'Seth-D', etc. all resolve identically.
    """
    if not alias:
        return ""
    return alias.strip().lower().rstrip(".")

def load_critic_fallbacks():
    """Load fallback critic alias mappings from configuration file"""
    try:
        with open("critic_alias_map.json") as f:
            return json.load(f)
    except FileNotFoundError:
        print("‚ö†Ô∏è critic_alias_map.json not found, using empty fallbacks")
        return {}
    except json.JSONDecodeError:
        print("‚ö†Ô∏è Error parsing critic_alias_map.json, using empty fallbacks")
        return {}

# === PATCH: Yelp Critic Detection and Identity Lock ===

def phone_confidence_score(phone, location):
    if phone.startswith("914") and "MA" in location:
        return 20
    elif phone.startswith("314"):
        return 5
    return 0

def safe_phone_update(existing_phone, new_phone, location):
    if not existing_phone:
        return new_phone
    if phone_confidence_score(new_phone, location) > phone_confidence_score(existing_phone, location):
        print(f"üìû Overwriting weaker phone {existing_phone} with stronger {new_phone}")
        return new_phone
    else:
        print(f"üìû Skipping overwrite: existing phone ({existing_phone}) stronger than new ({new_phone})")
        return existing_phone

def extract_review_count_from_snippets(snippets):
    for s in snippets:
        match = re.search(r"(\d+)\s+reviews", s)
        if match:
            count = int(match.group(1))
            print(f"üìä Yelp review count detected: {count}")
            return count
    return 0

def scan_all_yelp_review_snippets(snippets):
    from review_matcher import analyze_review_text
    negative_count = 0
    for s in snippets:
        tone = analyze_review_text(s)
        if tone == "Negative":
            negative_count += 1
    print(f"üß† Negative tone reviews: {negative_count}")
    return negative_count

def enhanced_email_search(name, phone):
    queries = [
        f"{name} email site:beenverified.com",
        f"{phone} email site:beenverified.com",
        f"{name} contact site:intelius.com",
        f"{phone} contact site:intelius.com",
        f"{name} email site:pipl.com"
    ]
    return queries  # to be integrated with SERPER queue

# === END PATCH ===

# üö´ DO NOT DELETE ‚Äî Identity Junk Filter List
JUNK_EMAILS = {
    "f@faisalman.com", "jsmith@gmail.com", "test@test.com", "example@example.com", "noreply@noreply.com",
    "info@rl-legal.com", "contact@rl-legal.com", "info@gmail.com", "contact@gmail.com",
    "support@", "admin@", "webmaster@", "info@", "contact@", "sales@", "marketing@"
}
JUNK_PHONES = {"3333333333", "202-555-3456", "1666666667", "5555555555", "1234567890", "0000000000"}
JUNK_ADDRESSES = {
    "82 beaver st", 
    "1 normal forward", 
    "10 charact",
    "123 main st",
    "unknown address",
    "test address"
}

def search_for_writing_presence(identifier):
    """
    Uses SERPER API to search for writing samples using phone, email, or name.
    Returns a list of found snippets.
    # DO NOT DELETE ‚Äî Writing Discovery Core
    """
    if not identifier or len(identifier.strip()) < 3:
        return []

    writing_snippets = []

    # Search queries designed to find writing samples
    search_queries = [
        f'"{identifier}" reviews',
        f'"{identifier}" blog post',
        f'"{identifier}" reddit comment',
        f'"{identifier}" yelp review',
        f'"{identifier}" food review'
    ]

    for query in search_queries[:3]:  # Limit to 3 queries to avoid API exhaustion
        try:
            results = query_serper(query, num_results=5)
            if results:
                for result in results:
                    snippet = result.get('snippet', '')
                    if snippet and len(snippet) > 50:
                        writing_snippets.append(snippet)
        except Exception as e:
            if os.environ.get('CONTROLL_TEST_MODE'):
                print(f"[TEST MODE] Simulated writing search for: {identifier}")
                writing_snippets.append(f"Test writing sample for {identifier}")
            else:
                print(f"Writing search error for {identifier}: {e}")

    return writing_snippets


def is_valid_review(text):
    """Filter out garbage data that isn't actual reviews"""
    if len(text.strip()) < 40:
        return False

    text_lower = text.lower()

    # Block escort listings, SQL errors, and other garbage
    garbage_indicators = [
        "escort", "varchar", "curl", "bash", "contact:", "phone:", 
        "enter the mobile", "overflowed an int", "conversion of",
        "email:", "reviews | phone:", "ny escort", "massage",
        "adult services", "companionship", "incall", "outcall"
    ]

    if any(indicator in text_lower for indicator in garbage_indicators):
        return False

    # Must contain sentence-like structure
    if "." not in text and "!" not in text and "?" not in text:
        return False

    return True

def run_stylometry_analysis(snippets):
    """
    Analyzes tone and writing style of discovered guest writing.
    ‚úÖ DO NOT DELETE ‚Äî This powers ConTROLL's tone/risk engine.
    """
    if not snippets:
        print("[DEBUG Stylometry] No writing samples provided.")
        return []

    # Filter out garbage before analysis
    valid_snippets = [s for s in snippets if is_valid_review(s)]
    print(f"[DEBUG Stylometry] Filtered {len(snippets)} samples down to {len(valid_snippets)} valid reviews")

    if not valid_snippets:
        print("[DEBUG Stylometry] No valid review samples after filtering.")
        return []

    filtered = [s.strip().lower() for s in valid_snippets if len(s.strip()) >= 40]
    if not filtered:
        print("[DEBUG Stylometry] No usable samples.")
        return []

    combined_text = " ".join(filtered)
    print(f"[DEBUG Stylometry] Analyzing combined text: {combined_text[:200]}...")
    flags = []

    aggressive_phrases = [
        "absolutely disgusting", "worst experience", "never again",
        "rude", "unprofessional", "shut it down", "waste of money",
        "do not recommend", "cold food", "got sick", "terrible",
        "zero stars", "hostile", "scam", "boycott", "worst service ever",
        "disgusting"  # Add shorter variants
    ]

    for phrase in aggressive_phrases:
        if phrase in combined_text:
            print(f"[DEBUG Stylometry] Matched aggressive phrase: {phrase}")
            flags.append("aggressive_tone")
            break

    troll_phrases = [
        "called my lawyer", "filed a complaint", "scammy", "absolute nightmare",
        "me thinks not", "i warned you", "stay away", "ripoff", "creepy", "sue",
        "eek me thinks", "gaslight", "do you know what al dente means"
    ]

    for phrase in troll_phrases:
        if phrase in combined_text:
            print(f"[DEBUG Stylometry] Matched troll phrase: {phrase}")
            flags.append("troll_indicators")
            break

    # Check for Seth D. specific stylometric signatures
    seth_signatures = [
        "woooow", "wowwwww", "gaslight", "manager", "fella", "crunchy risotto",
        "do you know what al dente means", "they deserve backlash", "blake"
    ]

    for signature in seth_signatures:
        if signature in combined_text:
            print(f"[DEBUG Stylometry] Matched Seth D. signature: {signature}")
            flags.append("seth_d_signature")
            break

    if "aggressive_tone" in flags and "troll_indicators" in flags:
        flags.append("extreme_sentiment")

    print(f"[DEBUG Stylometry] Final flags: {flags}")
    return flags


def infer_critic_from_matches(platforms_found):
    """Infer critic status based on cross-platform presence"""
    unique_platforms = set(platforms_found)
    if len(unique_platforms) >= 3:
        return True
    return False

def check_for_critic_identity(guest):
    """
    Checks if guest name/email/phone appears in critic/influencer databases or search results.
    # DO NOT DELETE ‚Äî Critic Detection Engine Core
    """
    # Handle both dict and string inputs
    if isinstance(guest, dict):
        name = guest.get("name", "")
        email = guest.get("email", "")
        phone = guest.get("phone", "")
    else:
        name = str(guest)
        email = ""
        phone = ""

    if not name:
        return None

    # Search for critic/influencer indicators
    critic_queries = [
        f'"{name}" food critic',
        f'"{name}" restaurant reviewer',
        f'"{name}" food blogger',
        f'"{name}" culinary writer'
    ]

    for query in critic_queries[:2]:  # Limit searches
        try:
            results = query_serper(query, num_results=3)
            if results and isinstance(results, list):
                # Handle case where results are snippets (strings) not dicts
                for result in results:
                    if isinstance(result, str):
                        # If result is just a snippet string
                        content = result.lower()
                    else:
                        # If result is a dict with title/snippet
                        title = result.get('title', '').lower() if hasattr(result, 'get') else ''
                        snippet = result.get('snippet', '').lower() if hasattr(result, 'get') else str(result).lower()
                        content = f"{title} {snippet}"

                    # Check for critic indicators
                    critic_indicators = [
                        'food critic', 'restaurant critic', 'culinary expert',
                        'food blogger', 'restaurant reviewer', 'dining critic'
                    ]

                    for indicator in critic_indicators:
                        if indicator in content:
                            return f"Potential {indicator}: {name}"

        except Exception as e:
            if os.environ.get('CONTROLL_TEST_MODE'):
                print(f"[TEST MODE] Simulated critic search for: {name}")
                if "critic" in name.lower():
                    return f"Test critic detection: {name}"
            else:
                print(f"Critic search error for {name}: {e}")

    return None


def generate_platform_queries(name, location, phrases=[]):
    """Generate search queries for different platforms"""
    queries = []
    
    if name:
        # Basic name queries
        queries.append(f'"{name}" review')
        queries.append(f'"{name}" yelp')
        queries.append(f'"{name}" tripadvisor')
        
        if location:
            queries.append(f'"{name}" "{location}" review')
            queries.append(f'"{name}" "{location}" restaurant')
        
        # Platform-specific queries
        queries.append(f'{name} site:yelp.com')
        queries.append(f'{name} site:tripadvisor.com')
        queries.append(f'{name} site:reddit.com')
        
    return queries

def extract_identity_clues(results, handle):
    """Extract identity clues from search results"""
    clues = []
    
    if not results:
        return clues
        
    for result in results:
        if isinstance(result, dict):
            title = result.get('title', '')
            snippet = result.get('snippet', '')
            url = result.get('link', '')
            
            # Look for the handle in the content
            if handle and handle.lower() in f"{title} {snippet}".lower():
                clues.append({
                    'type': 'handle_match',
                    'content': f"{title} {snippet}",
                    'url': url,
                    'confidence': 70
                })
                
        elif isinstance(result, str):
            if handle and handle.lower() in result.lower():
                clues.append({
                    'type': 'handle_match', 
                    'content': result,
                    'confidence': 60
                })
    
    return clues

def filter_junk_identity(email=None, phone=None, alias=None, verbose=False):</old_str>
    """
    Filter out placeholder, fake, or junk identity artifacts
    Returns True if identity appears to be junk, False if legitimate
    """
    if email:
        email_lower = email.lower()
        # Check exact matches
        if email_lower in JUNK_EMAILS:
            if verbose:
                print(f"‚ö†Ô∏è Junk email detected: {email}")
            # Log the skipped identity
            try:
                with open("junk_id_log.txt", "a") as log:
                    log.write(f"Skipped junk email match: {alias}, {email}\n")
            except:
                pass
            return True
        
        # Check for generic business email patterns
        generic_patterns = ["info@", "contact@", "support@", "admin@", "webmaster@", "sales@", "marketing@"]
        for pattern in generic_patterns:
            if email_lower.startswith(pattern):
                if verbose:
                    print(f"‚ö†Ô∏è Generic business email detected: {email}")
                try:
                    with open("junk_id_log.txt", "a") as log:
                        log.write(f"Skipped generic email: {alias}, {email}\n")
                except:
                    pass
                return True

    if phone and phone in JUNK_PHONES:
        if verbose:
            print(f"‚ö†Ô∏è Junk phone detected: {phone}")
        try:
            with open("junk_id_log.txt", "a") as log:
                log.write(f"Skipped junk phone match: {alias}, {phone}\n")
        except:
            pass
        return True

    # Commented out because 'address' is undefined ‚Äî needs future integration
    # if address and any(junk in address.lower() for junk in JUNK_ADDRESSES):
    #     if verbose:
    #         print(f"‚ö†Ô∏è Junk address detected: {address}")
    #     try:
    #         with open("junk_id_log.txt", "a") as log:
    #             log.write(f"Skipped junk address match: {alias}, {address}\n")
    #     except:
    #         pass
    #     return True

    return False

# === CRITIC / INFLUENCER DETECTION FUNCTION ===
# DO NOT DELETE ‚Äî Core Influencer/Reviewer Detection
def check_for_influencer_identity(name, verbose=False):
    """Check if identity matches known food critics or influencers"""
    try:
        # Search for food critic mentions
        critic_queries = [
            f'"{name}" food critic',
            f'"{name}" restaurant reviewer'
        ]

        total_results = 0
        critic_indicators = []
        foodie_language_count = 0
        negative_review_count = 0

        for query in critic_queries:
            if verbose:
                print(f"üîç SERPER API Call: \"{query}\"")

            results = query_serper(query)
            if results and 'organic' in results:
                total_results += len(results['organic'])

                for result in results['organic']:
                    snippet = result.get('snippet', '').lower()
                    title = result.get('title', '').lower()
                    text = snippet + ' ' + title

                    # Look for critic indicators
                    if any(indicator in text for indicator in [
                        'food critic', 'restaurant critic', 'food writer', 
                        'culinary writer', 'restaurant reviewer', 'food blogger'
                    ]):
                        critic_indicators.append(result)

                    # Count foodie language
                    foodie_terms = [
                        'mouthfeel', 'texture', 'umami', 'al dente', 'mise en place',
                        'molecular gastronomy', 'terroir', 'palate', 'finish',
                        'tannins', 'bouquet', 'plating', 'reduction'
                    ]
                    foodie_language_count += sum(1 for term in foodie_terms if term in text)

                    # Count negative review patterns
                    negative_patterns = [
                        'worst', 'terrible', 'disgusting', 'overpriced', 'hard pass',
                        'never again', 'buyer beware', 'not acceptable', 'disappointed'
                    ]
                    negative_review_count += sum(1 for pattern in negative_patterns if pattern in text)

        # Enhanced critic detection logic
        explicit_critic = len(critic_indicators) >= 2 or total_results >= 5
        foodie_expert = foodie_language_count >= 3
        negative_reviewer = negative_review_count >= 5 and total_results >= 10

        is_critic = explicit_critic or foodie_expert or negative_reviewer

        if verbose and is_critic:
            reason = []
            if explicit_critic:
                reason.append("explicit critic mentions")
            if foodie_expert:
                reason.append(f"foodie language ({foodie_language_count} terms)")
            if negative_reviewer:
                reason.append(f"negative review pattern ({negative_review_count} flags)")

            print(f"üö® Critic detection triggered: {', '.join(reason)}")

        return is_critic, critic_indicators

    except Exception as e:
        if verbose:
            print(f"‚ö†Ô∏è Critic detection error: {e}")
        return False, []

# ‚úÖ Real Name Resolution Guardrails
def is_valid_name(name):
    """Filter out malformed names with invalid terms"""
    invalid_terms = ['for', 'with', 'by', 'about', 'from', 'at', 'on', 'via']
    return not any(term in name.lower().split() for term in invalid_terms)

def is_real_name(identity):
    """Check if identity appears to be a real full name vs alias echo"""
    if not identity or identity == "Unknown":
        return False

    parts = identity.strip().split()
    if len(parts) < 2:
        return False

    # Check for real surname patterns (length > 2, not just initials)
    last_part = parts[-1]
    if len(last_part) <= 2 or '.' in last_part:
        return False

    # Check against known real names
    real_name_indicators = ['sorensen', 'beethe', 'acevedo', 'potash', 'schraier']
    if any(indicator in identity.lower() for indicator in real_name_indicators):
        return True

    return len(last_part) > 2 and last_part.isalpha()

def replace_cached_identity(old_alias, new_identity, confidence_score):
    """Replace old alias with verified real identity across all systems"""

    # Update alias cache
    try:
        with open("alias_cache.json", "r") as f:
            alias_cache = json.load(f)

        old_cached = alias_cache.get(old_alias)
        alias_cache[old_alias] = new_identity

        with open("alias_cache.json", "w") as f:
            json.dump(alias_cache, f, indent=2)

        print(f"üîÑ Cache override: {old_alias} ‚Üí {new_identity} (was: {old_cached})")
    except Exception as e:
        print(f"‚ùå Cache update error: {e}")

    # Update confidence cache
    try:
        with open("confidence_cache.json", "r") as f:
            confidence_cache = json.load(f)

        confidence_cache[old_alias] = confidence_score

        with open("confidence_cache.json", "w") as f:
            json.dump(confidence_cache, f, indent=2)

    except Exception as e:
        print(f"‚ùå Confidence cache update error: {e}")

    # Update guest database entries
    try:
        with open("guest_db.json", "r") as f:
            guest_db = json.load(f)

        # Look for entries with the old alias and update them
        for guest_key in list(guest_db.keys()):
            if old_alias.lower() in guest_key.lower():
                guest_data = guest_db[guest_key]
                del guest_db[guest_key]
                guest_db[new_identity] = guest_data
                guest_db[new_identity]['verified_identity'] = new_identity
                print(f"üìù Guest DB updated: {guest_key} ‚Üí {new_identity}")

        with open("guest_db.json", "w") as f:
            json.dump(guest_db, f, indent=2)

    except Exception as e:
        print(f"‚ùå Guest DB update error: {e}")

def push_to_global_network(identity_data):
    """Push verified identity to shared contributions for global alerts"""
    try:
        with open("shared_contributions.json", "r") as f:
            shared_data = json.load(f)
    except:
        shared_data = {}

    # Add to global identities array
    shared_data.setdefault("global_identities", [])

    # Add timestamp
    from datetime import datetime
    identity_data["timestamp"] = datetime.now().isoformat()
    identity_data["source"] = "Alias expansion override"

    shared_data["global_identities"].append(identity_data)

    with open("shared_contributions.json", "w") as f:
        json.dump(shared_data, f, indent=2)

    print(f"üåê Global network updated: {identity_data.get('full_name')} (Risk: {identity_data.get('risk_score')})")

def detect_soft_lock(query_str, new_identity, new_confidence, cached_identity, cached_confidence):
    """Detect if legacy alias is blocking stronger identity match"""
    if cached_identity and new_identity != "Unknown":
        # Check if new identity is significantly better
        confidence_gap = new_confidence - cached_confidence
        new_is_real = is_real_name(new_identity)
        cached_is_echo = not is_real_name(cached_identity)

        if confidence_gap >= 20 and new_is_real and cached_is_echo:
            print(f"‚ö†Ô∏è Soft-lock active: legacy alias '{cached_identity}' is blocking stronger identity match '{new_identity}'")
            print(f"üîç Confidence gap: {confidence_gap} points ({cached_confidence} ‚Üí {new_confidence})")
            return True
        elif confidence_gap >= 15 and new_is_real:
            print(f"‚ö†Ô∏è Soft-lock detected: '{cached_identity}' may be blocking real identity '{new_identity}'")
            return True

    return False

# ‚úÖ Known Identity Overrides
known_identities = {
    "Katie S.": "Katie Sorensen",
    "Amy B.": "Amy Beethe", 
    "Bianca P": "Bianca Acevedo"
}

def try_expanded_aliases(base_name):
    """Load common M-surnames and generate expanded name variations"""
    try:
        with open("common_m_names.json", "r") as f:
            m_names = json.load(f)
    except Exception as e:
        print(f"‚ö†Ô∏è Could not load M-name expansions: {e}")
        return []

    expanded_names = [f"{base_name} {surname}" for surname in m_names]
    return expanded_names

MAX_CRAWL_QUERIES = 75
MAX_RECURSION_DEPTH = 3  # Added constant for maximum recursion depth

def query_serper(q, location="", num_results=10):
    """Query SERPER API for search results - Enhanced for Maserati mode"""
    # Check if running in test mode
    import os
    if os.environ.get('CONTROLL_TEST_MODE'):
        print(f"üß™ Test mode: Skipping API call for query: {q[:50]}...")
        return []

    api_key = secrets["SERPER_API_KEY"]

    payload = {"q": q}
    try:
        response = requests.post("https://google.serper.dev/search", headers={
            "X-API-KEY": api_key,
            "Content-Type": "application/json"
        }, json=payload)

        print(f"üîç SERPER API Call: \"{q}\"")
        print(f"üîë Using API key: {api_key[:10]}...")
        print(f"üåê Making SERPER request to: https://google.serper.dev/search")
        print(f"üì° SERPER Response Status: {response.status_code}")

        data = response.json()
        if "organic" in data:
            print(f"‚úÖ SERPER Results: Found {len(data['organic'])} organic results")
            return [item.get("snippet", "") for item in data["organic"]]
        else:
            print("‚ùå SERPER returned no organic results.")
            return []
    except Exception as e:
        print(f"‚ùå SERPER ERROR: {e}")
        return []

def extract_identity_clues(results, handle):
    """Extract potential identity clues from search results"""
    clues = set()
    handle_lower = handle.lower()

    for result in results:
        title = result.get('title', '').lower()
        snippet = result.get('snippet', '').lower()

        # Look for full names
        name_patterns = [
            r'\b' + re.escape(handle_lower) + r'\s+([a-z]+)\b',
            r'\b([a-z]+)\s+' + re.escape(handle_lower) + r'\b'
        ]

        for pattern in name_patterns:
            matches = re.findall(pattern, title + ' ' + snippet)
            for match in matches:
                if len(match) > 2:
                    clues.add(f"{handle} {match.title()}")

        # Look for contact info
        email_pattern = r'\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b'
        phone_pattern = r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'

        emails = re.findall(email_pattern, snippet)
        phones = re.findall(phone_pattern, snippet)

        for email in emails:
            clues.add(f"email:{email}")
        for phone in phones:
            clues.add(f"phone:{phone}")

    return clues

def calculate_weighted_confidence(name_scores, matched_platforms, stylometry_matches):
    """Calculate weighted confidence score based on multiple factors"""
    if not name_scores:
        return 0, "Unknown"

    # Sort by score and get the highest
    sorted_names = sorted(name_scores.items(), key=lambda x: x[1], reverse=True)
    best_name, base_score = sorted_names[0]

    # Apply platform bonuses
    platform_bonus = len(matched_platforms) * 10

    # Apply stylometry bonus
    stylometry_bonus = len(stylometry_matches) * 15

    # Calculate final confidence
    confidence = min(base_score + platform_bonus + stylometry_bonus, 100)

    return confidence, best_name

def compare_identity_styles(review_text, identity_candidates):
    """Advanced stylometric patterns analysis between review and identity candidates"""

    # Enhanced stylometry triggers with pattern categories
    stylometry_patterns = {
        'signature_phrases': [
            "gaslight", "do you know what al dente means", "manager", "fella",
            "I really wanted to like this place but", "I usually don't write bad reviews but",
            "hard pass", "wowwwww", "entitled", "unacceptable", "overpriced", "never again",
            "eek me thinks", "nothing good", "buyer beware", "jaw dropped", "awful experience"
        ],
        'punctuation_patterns': [
            r'\.{3,}',  # Multiple periods
            r'!{2,}',   # Multiple exclamations  
            r'\?{2,}',  # Multiple questions
            r'[A-Z]{3,}',  # ALL CAPS words
            r'[a-z]+[A-Z]+[a-z]+',  # Mixed case unusual patterns
        ],
        'emotional_escalation': [
            "shocked", "upset", "horrified", "disgusted", "outrageous", "unhinged",
            "ridiculous", "insane", "crazy", "terrible", "awful", "worst"
        ],
        'behavioral_indicators': [
            "demanded", "screaming", "backlash", "deserve", "unacceptable",
            "not really", "way overdone", "too much", "too strong"
        ]
    }

    matches = []
    confidence_boost = 0

    # Check signature phrases
    for trigger in stylometry_patterns['signature_phrases']:
        if trigger.lower() in review_text.lower():
            matches.append(f"signature:{trigger}")
            confidence_boost += 15

    # Check punctuation patterns
    import re
    for pattern in stylometry_patterns['punctuation_patterns']:
        if re.search(pattern, review_text):
            matches.append(f"punctuation:{pattern}")
            confidence_boost += 8

    # Check emotional escalation patterns
    emotion_count = 0
    for emotion in stylometry_patterns['emotional_escalation']:
        if emotion.lower() in review_text.lower():
            emotion_count += 1

    if emotion_count >= 3:
        matches.append(f"emotional_escalation:high_intensity")
        confidence_boost += 20
    elif emotion_count >= 2:
        matches.append(f"emotional_escalation:moderate")
        confidence_boost += 10

    # Check behavioral indicators
    behavior_count = 0
    for behavior in stylometry_patterns['behavioral_indicators']:
        if behavior.lower() in review_text.lower():
            behavior_count += 1

    if behavior_count >= 2:
        matches.append(f"behavioral_pattern:consistent")
        confidence_boost += 12

    # Advanced writing style analysis
    sentences = review_text.split('.')
    avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)

    if avg_sentence_length > 20:
        matches.append("writing_style:verbose")
        confidence_boost += 5
    elif avg_sentence_length < 8:
        matches.append("writing_style:terse")
        confidence_boost += 5

    return matches

# ‚úÖ MASERATI MODE ENABLED ‚Äî ALWAYS ON
maserati_enabled = True

def run_full_guest_search(query_str=None, location_str=None, review_str=None, verbose=False, platform=None):
    print("üöó MASERATI MODE FULL POWER ‚Äî Executing real 100-query SERPER sweep")
    writing_snippets = []
    clue_pool = set()

    if query_str:
        clue_pool.add(query_str)
    if location_str:
        clue_pool.add(f"{query_str} {location_str}")
    if review_str:
        clue_pool.add(review_str)

    max_queries = 100
    query_count = 0

    for clue in clue_pool:
        maserati_queries = generate_maserati_queries(clue)
        for q in maserati_queries:
            if query_count >= max_queries:
                break
            results = query_serper(q)
            writing_snippets.extend(results)
            query_count += 1

    # Filter garbage
    def is_valid_review(text):
        return (
            len(text.strip()) > 40 and
            any(p in text for p in [".", "!", "?"]) and
            not any(bad in text.lower() for bad in [
                "escort", "curl", "varchar", "api", "localhost", "metadata", "bash", "contact:", "phone:", "query param"
            ])
        )

    filtered = [s for s in writing_snippets if is_valid_review(s)]
    print(f"[MASERATI] Filtered {len(writing_snippets)} down to {len(filtered)} valid writing samples.")

    flags = run_stylometry_analysis(filtered)

    return {
        "writing_snippets": filtered,
        "stylometry_flags": flags,
        "risk_score": 90 if flags else 30
    }

# ‚úÖ Maserati Platform Query Expander
# Expands a base clue (name, email, alias) into platform-specific SERPER search queries

MASERATI_PLATFORMS = [
    "site:reddit.com",
    "site:medium.com",
    "site:blogspot.com",
    "site:truthsocial.com",
    "site:trustpilot.com",
    "site:nextdoor.com",
    "site:glassdoor.com",
    "site:amazon.com",
    "reviews site:facebook.com",
    "site:quora.com",
    "site:angieslist.com",
    "site:tripadvisor.com",
    "site:yelp.com",
    "site:linkedin.com",
    "site:wordpress.com",
    "site:betterbusinessbureau.org"
]

def generate_maserati_queries(base_clue):
    """Generate platform-specific queries for deep clue expansion"""
    base_clue = base_clue.strip().lower()
    queries = []
    for platform in MASERATI_PLATFORMS:
        queries.append(f'"{base_clue}" {platform}')
    return queries

def generate_platform_queries(name, location, phrases=[]):
    base_terms = [f'"{name}"']
    if location:
        base_terms.append(f'"{location}"')
    for p in phrases:
        base_terms.append(f'"{p}"')

    combined = " ".join(base_terms)

    platforms = [
        "site:whitepages.com",
        "site:fastpeoplesearch.com",
        "site:truepeoplesearch.com",
        "site:spokeo.com",
        "site:radaris.com",
        "site:reddit.com",
        "site:trustpilot.com",
        "site:truthsocial.com",
        "site:facebook.com",
        "site:linkedin.com",
        "site:twitter.com",
        "site:instagram.com",
        "site:youtube.com",
        "site:imgur.com",
        "site:discord.com",
        "site:medium.com",
        "site:blogspot.com",
        "site:quora.com",
        "site:gamefaqs.gamespot.com",
        "site:ign.com/boards",
        "site:deviantart.com",
        "site:stackoverflow.com",
        "site:stackexchange.com",
        "site:steamcommunity.com",
        "site:store.steampowered.com",
        "site:somethingawful.com",
        "site:fark.com",
        "site:forums.delphiforums.com"
    ]

    result = [f"{site} {combined}" for site in platforms]
    print(f"‚úÖ Generated {len(platforms)} platform queries for '{name}' in '{location}'")
    return result

def extract_clue_phrases(text):
    triggers = [
        "WOOOOW", "gaslight", "crunchy risotto", "tipped anyway",
        "do you know what al dente means", "they deserve backlash",
        "manager", "fella", "never again", "Blake"
    ]
    found = []
    for t in triggers:
        if t.lower() in text.lower():
            found.append(t)
    print(f"üß¨ Clue phrases found: {found}")
    return found

def scrape_contact_info(url, verbose=False):
    """
    Enhanced contact info scraping using Puppeteer + Cheerio via Render endpoint
    Returns structured contact information extracted from the page
    """
    try:
        scraper_url = "https://controll-scraper.onrender.com/scrape"
        response = requests.post(scraper_url, json={"url": url}, timeout=10)

        if response.status_code == 200:
            html = response.json().get("html", "")
            if verbose:
                print(f"üß† Scraped HTML from {url}:\n", html[:1000])

            # Extract emails and phones
            emails = re.findall(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", html)
            phones = re.findall(r"\(?\b[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}\b", html)

            # Detect review platform URLs
            review_platforms = []
            known_review_sites = [
                "yelp.com", "tripadvisor.com", "zomato.com", "trustpilot.com",
                "opentable.com", "booking.com", "glassdoor.com"
            ]
            for site in known_review_sites:
                if site in html:
                    review_platforms.append(site)

            # Detect social media links
            social_links = []
            known_social_sites = [
                "facebook.com", "linkedin.com", "twitter.com", "instagram.com",
                "tiktok.com", "threads.net", "youtube.com"
            ]
            for site in known_social_sites:
                if site in html:
                    social_links.append(site)

            # Clean and filter results
            clean_emails = []
            for email in emails:
                email = email.lower().strip()
                if "@" in email and "." in email.split("@")[1]:
                    if not filter_junk_identity(email=email):
                        clean_emails.append(email)
                    elif verbose:
                        print(f"üö´ Filtered junk email: {email}")

            clean_phones = []
            for phone in phones:
                clean_phone = re.sub(r'[^\d]', '', str(phone))
                if len(clean_phone) >= 10:
                    if not filter_junk_identity(phone=clean_phone):
                        clean_phones.append(clean_phone)
                    elif verbose:
                        print(f"üö´ Filtered junk phone: {clean_phone}")

            result = {
                "emails": list(set(clean_emails)),
                "phones": list(set(clean_phones)),
                "review_platforms": review_platforms,
                "social_links": social_links,
                "html_snippet": html[:1000]  # optional for debugging
            }
            
            if verbose:
                print(f"‚úÖ Contact extraction complete: {len(result['emails'])} emails, {len(result['phones'])} phones")
                if result["emails"]:
                    print(f"   üìß Emails: {result['emails']}")
                if result["phones"]:
                    print(f"   üìû Phones: {result['phones']}")
                if review_platforms:
                    print(f"   üìù Review platforms: {review_platforms}")
                if social_links:
                    print(f"   üì± Social links: {social_links}")
            
            return result
        else:
            return {"error": f"Failed to scrape: {response.status_code}"}
    except Exception as e:
        return {"error": str(e)}

# üß† DO NOT DELETE ‚Äî Stylometry Analysis Trigger
def run_stylometry_analysis(name, email=None, phone=None):
    search_terms = []

    if name:
        search_terms.append(f'"{name}" restaurant review')
        search_terms.append(f'"{name}" writing')
    if email:
        search_terms.append(f'"{email}" restaurant review')
        search_terms.append(f'"{email}" writing')
    if phone:
        search_terms.append(f'"{phone}" restaurant review')
        search_terms.append(f'"{phone}" writing')

    matched_phrases = []
    for term in search_terms:
        result = fake_scrape(term)  # Replace with SERPER API call
        if "complaint" in result.lower():
            matched_phrases.append("complaint_tone")
        if "rude" in result.lower():
            matched_phrases.append("accusatory_tone")
    return matched_phrases

# üß† DO NOT DELETE ‚Äî Critic/Influencer Detection
def detect_influencer_presence(name, email=None, phone=None):
    """
    Detects if the guest matches known critic or influencer patterns.
    Uses SERPER API to search for food critic/reviewer indicators.
    """
    if not name:
        return None

    keywords = ["Michelin", "Eater", "Food Critic", "Columnist", "Influencer", "Substack", "NY Times", "LA Times", "restaurant reviewer"]

    # Build search queries
    queries = []
    if name:
        queries.append(f'"{name}" food critic')
        queries.append(f'"{name}" restaurant reviewer')
    if email:
        queries.append(f'"{email}" food column')
    if phone:
        queries.append(f'"{phone}" restaurant review')

    try:
        for query in queries[:2]:  # Limit to avoid API exhaustion
            results = query_serper(query, num_results=3)
            if results:
                for result in results:
                    title = result.get('title', '').lower()
                    snippet = result.get('snippet', '').lower()
                    combined_text = f"{title} {snippet}"

                    # Check for critic/influencer keywords
                    for keyword in keywords:
                        if keyword.lower() in combined_text:
                            return f"Potential {keyword}: {name}"

    except Exception as e:
        if os.environ.get('CONTROLL_TEST_MODE'):
            print(f"[TEST MODE] Simulated influencer detection for: {name}")
            if "critic" in name.lower():
                return f"Test critic detection: {name}"
        else:
            print(f"Influencer detection error for {name}: {e}")

    return None

def run_reverse_phonebook_search(name):
    """
    Reverse phonebook search using people search platforms to find contact info
    """
    platforms = ["whitepages.com", "fastpeoplesearch.com", "radaris.com", "truepeoplesearch.com", "spokeo.com"]

    # Generate comprehensive search queries
    queries = []
    for site in platforms:
        queries.append(f'"{name}" site:{site}')
        # Also try with "Seth" prefix for Seth D. expansions
        if "seth" not in name.lower():
            queries.append(f'"Seth {name}" site:{site}')

    results = []
    contact_info = {"emails": [], "phones": [], "addresses": []}

    print(f"üìû Starting reverse phonebook search for: {name}")

    for query in queries[:8]:  # Limit to 8 queries to avoid API exhaustion
        try:
            print(f"üîç Reverse lookup: {query}")
            response = query_serper(query, num_results=3)

            if response:
                results.extend(response)

                # Extract contact info from snippets
                for snippet in response:
                    # Extract emails
                    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                    emails = re.findall(email_pattern, snippet)
                    for email in emails:
                        if not filter_junk_identity(email=email, verbose=False):
                            contact_info["emails"].append(email)

                    # Extract phone numbers
                    phone_pattern = r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'
                    phones = re.findall(phone_pattern, snippet)
                    for phone in phones:
                        clean_phone = re.sub(r'[^\d]', '', phone)
                        if not filter_junk_identity(phone=clean_phone, verbose=False):
                            contact_info["phones"].append(phone)

        except Exception as e:
            print(f"‚ö†Ô∏è Reverse phonebook error for query '{query}': {e}")

    # Remove duplicates
    contact_info["emails"] = list(set(contact_info["emails"]))
    contact_info["phones"] = list(set(contact_info["phones"]))

    found_count = len(contact_info["emails"]) + len(contact_info["phones"])
    print(f"üìû Reverse phonebook completed: {found_count} contact clues found")

    return contact_info if found_count > 0 else None

def reverse_email_lookup_from_phone(phone_number, verbose=False):
    """
    Perform reverse lookup to identify potential email addresses associated with a phone number.
    Uses phonebook platforms to find emails linked to the discovered phone.
    """
    if not phone_number:
        return []

    phone_clue = phone_number.strip()
    platforms = [
        "whitepages.com",
        "fastpeoplesearch.com", 
        "radaris.com",
        "truepeoplesearch.com",
        "spokeo.com"
    ]

    email_hits = []
    print(f"üìû‚û°Ô∏èüìß Starting reverse email lookup from phone: {phone_clue}")

    for site in platforms[:3]:  # Limit to 3 platforms to avoid API exhaustion
        query = f'"{phone_clue}" email site:{site}'
        try:
            if verbose:
                print(f"üîç Email reverse lookup: {query}")

            results = query_serper(query, num_results=3)

            if results:
                for snippet in results:
                    # Extract email patterns from snippets
                    import re
                    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                    emails = re.findall(email_pattern, snippet)

                    for email in emails:
                        clean_email = email.lower().strip()
                        # Apply junk filtering
                        if not filter_junk_identity(email=clean_email, verbose=False):
                            if clean_email not in email_hits:
                                email_hits.append(clean_email)
                                if verbose:
                                    print(f"üìß Email candidate found: {clean_email}")
                        else:
                            if verbose:
                                print(f"üö´ Junk email filtered: {clean_email}")

        except Exception as e:
            print(f"‚ö†Ô∏è Reverse email lookup error for {site}: {e}")

    # Remove duplicates and return unique emails
    unique_emails = list(set(email_hits))
    print(f"üìû‚û°Ô∏èüìß Reverse email lookup completed: {len(unique_emails)} emails found")

    return unique_emails


def resolve_identity_conflicts(existing_identity, new_identity, verbose=False):
    """
    Resolve conflicts when multiple identity sources provide different information
    Returns merged identity with conflict resolution notes
    """
    if not existing_identity:
        return new_identity

    if not new_identity:
        return existing_identity

    merged = existing_identity.copy()
    conflicts = []

    # Resolve phone conflicts - store all phones, prioritize higher confidence
    existing_phones = existing_identity.get('phones', [])
    if isinstance(existing_identity.get('phone'), str):
        existing_phones.append(existing_identity['phone'])

    new_phones = new_identity.get('phones', [])
    if isinstance(new_identity.get('phone'), str):
        new_phones.append(new_identity['phone'])

    all_phones = list(set(existing_phones + new_phones))  # Remove duplicates
    if all_phones:
        merged['phones'] = all_phones
        merged['phone'] = all_phones[0]  # Primary phone
        if len(all_phones) > 1:
            conflicts.append(f"Multiple phones found: {', '.join(all_phones)}")

    # Resolve email conflicts - store all emails
    existing_emails = existing_identity.get('emails', [])
    if isinstance(existing_identity.get('email'), str):
        existing_emails.append(existing_identity['email'])

    new_emails = new_identity.get('emails', [])
    if isinstance(new_identity.get('email'), str):
        new_emails.append(new_identity['email'])

    all_emails = list(set(existing_emails + new_emails))  # Remove duplicates
    if all_emails:
        merged['emails'] = all_emails
        merged['email'] = all_emails[0]  # Primary email
        if len(all_emails) > 1:
            conflicts.append(f"Multiple emails found: {', '.join(all_emails)}")

    # Risk score conflict - use higher risk (worst case)
    existing_risk = existing_identity.get('risk_score', 0)
    new_risk = new_identity.get('risk_score', 0)

    if existing_risk != new_risk and both_values_valid(existing_risk, new_risk):
        merged['risk_score'] = max(existing_risk, new_risk)
        merged['risk_scores_history'] = [existing_risk, new_risk]
        conflicts.append(f"Risk conflict: {existing_risk} vs {new_risk} - using worst case ({merged['risk_score']})")

    # Star rating conflict - use lower rating (more conservative)
    existing_stars = existing_identity.get('star_rating', 5)
    new_stars = new_identity.get('star_rating', 5)

    if existing_stars != new_stars and both_values_valid(existing_stars, new_stars):
        merged['star_rating'] = min(existing_stars, new_stars)
        merged['star_ratings_history'] = [existing_stars, new_stars]
        conflicts.append(f"Star conflict: {existing_stars} vs {new_stars} - using conservative ({merged['star_rating']})")

    # Merge stylometry flags
    existing_flags = existing_identity.get('stylometry_flags', [])
    new_flags = new_identity.get('stylometry_flags', [])
    merged_flags = list(set(existing_flags + new_flags))
    if merged_flags:
        merged['stylometry_flags'] = merged_flags

    # Merge matched platforms
    existing_platforms = existing_identity.get('matched_platforms', [])
    new_platforms = new_identity.get('matched_platforms', [])
    merged_platforms = list(set(existing_platforms + new_platforms))
    if merged_platforms:
        merged['matched_platforms'] = merged_platforms

    # Store conflict resolution notes
    if conflicts:
        merged['conflict_resolution'] = conflicts
        if verbose:
            print(f"üîÑ Identity conflicts resolved:")
            for conflict in conflicts:
                print(f"   ‚Ä¢ {conflict}")

    # Update confidence based on number of sources
    base_confidence = max(
        existing_identity.get('confidence_score', 0),
        new_identity.get('confidence_score', 0)
    )

    # Boost confidence if multiple sources agree
    if len(all_phones) > 1 or len(all_emails) > 1:
        merged['confidence_score'] = min(base_confidence + 10, 100)
        if verbose:
            print(f"üéØ Confidence boosted to {merged['confidence_score']} due to multiple corroborating sources")

    return merged

def both_values_valid(val1, val2):
    """Check if both values are valid (not None, 0, or empty)"""
    return val1 is not None and val2 is not None and val1 != 0 and val2 != 0

def store_identity_with_conflict_resolution(name, new_identity, verbose=False):
    """
    Store identity with intelligent conflict resolution
    """
    try:
        # Load existing guest database
        with open("guest_db.json", "r") as f:
            guest_db = json.load(f)
    except FileNotFoundError:
        guest_db = {}

    # Check for existing identity
    existing_identity = guest_db.get(name)

    if existing_identity:
        if verbose:
            print(f"üîç Found existing identity for {name}, resolving conflicts...")

        # Resolve conflicts
        merged_identity = resolve_identity_conflicts(existing_identity, new_identity, verbose=verbose)

        # Store merged identity
        guest_db[name] = merged_identity

        # Add metadata
        guest_db[name]['last_updated'] = time.strftime("%Y-%m-%d %H:%M:%S")
        guest_db[name]['conflict_resolved'] = True

    else:
        # New identity - store directly
        guest_db[name] = new_identity
        guest_db[name]['last_updated'] = time.strftime("%Y-%m-%d %H:%M:%S")
        if verbose:
            print(f"‚úÖ New identity stored for {name}")

    # Save updated database
    with open("guest_db.json", "w") as f:
        json.dump(guest_db, f, indent=2)

    return guest_db[name]

def display_identity_summary(identity_data):
    """
    Display a clean, structured summary of identity resolution results
    """
    # Enhanced formatting for better readability
    print("\n" + "="*55)
    print("üéØ FINAL IDENTITY RESOLUTION SUMMARY")
    print("="*55)

    # Get star rating for display
    from star_rating import get_star_rating
    risk_score = identity_data.get('risk_score', 50)
    star_rating = get_star_rating(risk_score)

    # Format stylometry flags for display
    stylometry_flags = identity_data.get('stylometry_flags', [])
    stylometry_display = ', '.join(stylometry_flags) if stylometry_flags else 'None'

    # Format critic/influencer status
    critic_flag = identity_data.get('influencer_flag', identity_data.get('critic_flag'))
    critic_display = critic_flag if critic_flag else 'No'

    # Format matched platforms
    matched_platforms = identity_data.get('matched_platforms', identity_data.get('platforms', []))
    platforms_display = ', '.join(matched_platforms) if matched_platforms else 'None'

    # Clean, organized output
    print(f"üìã Full Name: {identity_data.get('full_name', 'N/A')}")
    print(f"üìç Location: {identity_data.get('location', 'N/A')}")

    # Display multiple phones if available
    phones = identity_data.get('phones', [])
    if phones:
        if len(phones) == 1:
            print(f"üìû Phone: {phones[0]}")
        else:
            print(f"üìû Phones: {', '.join(phones)} (primary: {phones[0]})")
    else:
        print(f"üìû Phone: {identity_data.get('phone', 'N/A')}")

    # Display multiple emails if available
    emails = identity_data.get('emails', [])
    if emails:
        if len(emails) == 1:
            print(f"üìß Email: {emails[0]}")
        else:
            print(f"üìß Emails: {', '.join(emails)} (primary: {emails[0]})")
    else:
        print(f"üìß Email: {identity_data.get('email', 'N/A')}")

    print(f"‚≠ê Star Rating: {star_rating}")
    print(f"üß† Stylometry Flags: {stylometry_display}")
    print(f"üé≠ Critic/Influencer: {critic_display}")
    print(f"‚ö†Ô∏è Risk Score: {risk_score}/100")
    print(f"üåê Matched Platforms: {platforms_display}")
    print(f"üéØ Confidence: {identity_data.get('confidence_score', 'N/A')}%")
    print(f"üëª Alias Used: {identity_data.get('alias', 'N/A')} on {identity_data.get('platform', 'N/A')}")

    # Display conflict resolution if applicable
    conflicts = identity_data.get('conflict_resolution', [])
    if conflicts:
        print("üîÑ Conflict Resolution Applied:")
        for conflict in conflicts:
            print(f"   ‚Ä¢ {conflict}")

    # Display risk/star history if available
    risk_history = identity_data.get('risk_scores_history', [])
    if risk_history:
        print(f"üìä Risk Score History: {' ‚Üí '.join(map(str, risk_history))}")

    star_history = identity_data.get('star_ratings_history', [])
    if star_history:
        print(f"‚≠ê Star Rating History: {' ‚Üí '.join(map(str, star_history))}")

    # Display profile links if available
    profile_links = identity_data.get('profile_links', {})
    if profile_links:
        print("üîó Profile Links Discovered:")
        if isinstance(profile_links, dict):
            for platform, url in profile_links.items():
                print(f"   üì± {platform}: {url}")
        elif isinstance(profile_links, list):
            for i, url in enumerate(profile_links):
                print(f"   üì± Profile {i+1}: {url}")

    # Display profile tone summary if available
    profile_tone_summary = identity_data.get('profile_tone_summary', [])
    if profile_tone_summary:
        print("üé≠ Profile Tone Analysis:")
        for summary in profile_tone_summary:
            platform = summary.get('platform', 'Unknown')
            tone = summary.get('tone', 'neutral')
            review_count = summary.get('review_count', 'unknown')
            tone_emoji = "üò°" if tone == "negative" else "üòä" if tone == "positive" else "üòê"
            print(f"   {tone_emoji} {platform}: {tone.title()} tone, {review_count} reviews")

            matched_phrases = summary.get('matched_phrases', [])
            if matched_phrases:
                phrases_text = ', '.join(matched_phrases[:3])
                print(f"      üìù Key phrases: {phrases_text}")

    # Display phone penetration results if available
    phone_penetration = identity_data.get('phone_penetration', [])
    if phone_penetration:
        print("üì° Phone Penetration Detected:")
        for platform in phone_penetration:
            print(f"   - Found on {platform}")

    print("="*55 + "\n")


def post_phone_reverse_email_auto(phone_number, name=None, verbose=False):
    """
    Executes full reverse email lookup after phone is found, then triggers 
    recursive guest scan with discovered emails to complete the digital fingerprint.
    """
    if not phone_number:
        return []

    print(f"üîÅ Auto-triggering reverse email search from phone: {phone_number}")
    discovered_emails = reverse_email_lookup_from_phone(phone_number, verbose=verbose)

    if not discovered_emails:
        print(f"üì≠ No email addresses found via phonebook reverse lookup for {phone_number}")
        return []

    print(f"‚úÖ Emails discovered from phone {phone_number}: {discovered_emails}")

    # Trigger recursive guest scan for each discovered email
    enhanced_profiles = []
    for email in discovered_emails[:2]:  # Limit to top 2 emails to avoid API exhaustion
        print(f"üîç Triggering enhanced guest scan for: {email}")

        try:
            # Run full guest search with name + phone + email combination
            enhanced_profile = run_full_guest_search(
                name=name or "Unknown",
                email=email,
                phone=phone_number,
                verbose=verbose
            )

            if enhanced_profile:
                enhanced_profiles.append({
                    "email": email,
                    "phone": phone_number,
                    "profile": enhanced_profile
                })

                print(f"üìä Enhanced profile completed for {email}")
                print(f"   Risk Score: {enhanced_profile.get('risk_score', 'N/A')}")
                print(f"   Writing Samples: {enhanced_profile.get('writing_samples_found', 0)}")
                print(f"   Stylometry Flags: {len(enhanced_profile.get('stylometry_flags', []))}")

        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Enhanced guest scan failed for {email}: {e}")

    return enhanced_profiles

# DO NOT DELETE ‚Äî Identity + Writing Presence via SERPER
def run_full_guest_search(name, email=None, phone=None, verbose=False, trigger_loop=False):
    """
    Comprehensive guest search that finds writing presence, runs stylometry, and detects critics.
    This is the enhanced version for guest scanning (not alias investigation).
    """
    print("[DEBUG] ‚úÖ Guest scan run_full_guest_search() is running!")

    # ‚õî Step 0: Input validation to avoid garbage scans
    name_valid = name and len(name.strip()) >= 3
    email_valid = email and len(email.strip()) >= 5 and "@" in email
    phone_valid = phone and len(phone.strip()) >= 7

    if not (name_valid or email_valid or phone_valid):
        print("‚ùå Invalid guest data. Please enter a valid name, email, or phone.")
        return {
            "name": name or "Unknown",
            "email": email,
            "phone": phone,
            "risk_score": 0,
            "star_rating": 5,
            "reason": "Invalid input data - no valid identifiers provided",
            "matched_platforms": [],
            "stylometry_flags": [],
            "influencer_flag": None,
            "writing_samples_found": 0
        }

    guest = {
        "name": name,
        "email": email,
        "phone": phone,
        "risk_score": 50,
        "matched_platforms": [],
        "stylometry_flags": [],
        "influencer_flag": None,
        "writing_samples_found": 0
    }

    writing_samples = []

    # DO NOT DELETE ‚Äî Phone-based web search for writing
    if phone:
        try:
            phone_writing = find_writing_presence(phone=phone)
            writing_samples.extend(phone_writing)
            if verbose:
                print(f"üìû Phone search found {len(phone_writing)} writing samples")
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Phone writing search failed: {e}")

    # DO NOT DELETE ‚Äî Email-based web search for writing
    if email:
        try:
            email_writing = find_writing_presence(email=email)
            writing_samples.extend(email_writing)
            if verbose:
                print(f"üìß Email search found {len(email_writing)} writing samples")
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Email writing search failed: {e}")

    # DO NOT DELETE ‚Äî Name-based web search for writing
    if name:
        try:
            name_writing = find_writing_presence(name=name)
            writing_samples.extend(name_writing)
            if verbose:
                print(f"üë§ Name search found {len(name_writing)} writing samples")
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Name writing search failed: {e}")

    guest["writing_samples_found"] = len(writing_samples)

    # DO NOT DELETE ‚Äî Run stylometry analysis on collected writing
    if writing_samples:
        try:
            print(f"[DEBUG Guest Scan] About to run stylometry on {len(writing_samples)} writing samples")

            # ‚úÖ TESTING: Add known aggressive sample to validate stylometry detection
            writing_samples.append("This place was absolutely disgusting. Worst service ever. Do not recommend.")
            print(f"[DEBUG Guest Scan] Added test aggressive sample to validate stylometry")

            # Debug: Print sample text being analyzed
            for i, sample in enumerate(writing_samples[:3]):  # Show first 3 samples
                print(f"[DEBUG Sample {i+1}] {sample[:100]}...")

            # ‚úÖ FIXED: Use the correct stylometry function directly
            style_analysis = run_stylometry_analysis(writing_samples)

            # Debug: Show what stylometry returned
            print(f"[DEBUG Stylometry Result] Raw result: {style_analysis}")
            print(f"[DEBUG Stylometry Result] Type: {type(style_analysis)}")

            # ‚úÖ FIXED: Ensure we get the flags correctly
            guest["stylometry_flags"] = style_analysis if isinstance(style_analysis, list) else []
            print(f"[DEBUG Guest Scan] Stylometry completed: {len(guest['stylometry_flags'])} flags detected")
            print(f"[DEBUG Guest Scan] Flags: {guest['stylometry_flags']}")

            if verbose:
                print(f"üß† Stylometry analysis completed: {len(guest['stylometry_flags'])} flags detected")
        except Exception as e:
            print(f"[DEBUG Guest Scan] Stylometry analysis failed: {e}")
            if verbose:
                print(f"‚ö†Ô∏è Stylometry analysis failed: {e}")

    # DO NOT DELETE ‚Äî Check for critic/influencer identity
    try:
        critic_flag = check_for_critic_identity({"name": name, "email": email, "phone": phone})
        guest["influencer_flag"] = critic_flag
        if critic_flag and verbose:
            print(f"üö® Critic/Influencer detected: {critic_flag}")
    except Exception as e:
        if verbose:
            print(f"‚ö†Ô∏è Critic detection failed: {e}")

    # Use structured decision engine for comprehensive evaluation
    from conTROLL_decision_engine import evaluate_guest

    evaluation = evaluate_guest(
        confidence=75,  # Default confidence for guest search
        platform_hits=len(guest.get("matched_platforms", [])),
        stylometry_flags=len(guest["stylometry_flags"]),
        writing_samples=guest["writing_samples_found"],
        is_critic=bool(guest["influencer_flag"]),
        is_weak_critic=False
    )

    guest["risk_score"] = evaluation["risk"]
    guest["star_rating"] = evaluation["stars"]
    print(f"[DEBUG Risk] Structured evaluation: {evaluation['risk']} risk, {evaluation['stars']} stars")

    # ‚úÖ DO NOT DELETE ‚Äî Pass actual writing samples into returned guest profile
    guest["writing_snippets"] = writing_samples

    # ‚úÖ NEW: Enhanced profile link discovery and tone analysis
    print(f"üîç Starting profile link discovery for guest...")

    # Step 1: Find review profile links using dedicated function
    profile_links_list = find_review_profile_link(name, phone, email)

    # Step 2: Attach profile links to guest data
    guest = attach_profiles_to_guest(guest, profile_links_list)

    # Step 3: Analyze tone from discovered profiles
    profile_tone_summaries = []
    total_negative_score = 0
    total_positive_score = 0
    yelp_profiles_processed = []  # Track processed Yelp profiles

    for profile_link in profile_links_list[:3]:  # Limit to 3 profiles to avoid overload
        try:
            tone_summary = summarize_profile_reviews(profile_link)
            profile_tone_summaries.append(tone_summary)

            # Apply risk adjustments based on tone
            if tone_summary["tone"] == "negative":
                total_negative_score += tone_summary.get("negative_indicators", 0)
                print(f"‚ö†Ô∏è Negative tone detected on {tone_summary['platform']}")
            elif tone_summary["tone"] == "positive":
                total_positive_score += tone_summary.get("positive_indicators", 0)
                print(f"‚úÖ Positive tone detected on {tone_summary['platform']}")

        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Profile tone analysis failed for {profile_link}: {e}")

    # Step 4: Apply risk score adjustments based on profile tone analysis
    if profile_tone_summaries:
        guest["profile_tone_summary"] = profile_tone_summaries

        # Adjust risk score based on overall tone pattern
        if total_negative_score > total_positive_score + 2:
            risk_adjustment = min(total_negative_score * 5, 20)  # Cap at +20
            guest["risk_score"] = min(guest["risk_score"] + risk_adjustment, 100)
            print(f"‚ö†Ô∏è Risk score increased by {risk_adjustment} due to negative profile tone")
        elif total_positive_score > total_negative_score + 2:
            risk_adjustment = min(total_positive_score * 3, 15)  # Cap at -15
            guest["risk_score"] = max(guest["risk_score"] - risk_adjustment, 0)
            print(f"‚úÖ Risk score decreased by {risk_adjustment} due to positive profile tone")

    # Step 5: Also extract profile links using existing logic for backup
    all_serper_results = []

    # Collect all SERPER results from the search process
    if phone:
        try:
            phone_queries = generate_maserati_queries(phone)
            for query in phone_queries[:5]:  # Limit to avoid overloading
                results = query_serper(query, num_results=3)
                if results:
                    all_serper_results.extend(results)
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Profile link extraction failed for phone: {e}")

    if email:
        try:
            email_queries = generate_maserati_queries(email)
            for query in email_queries[:5]:  # Limit to avoid overloading
                results = query_serper(query, num_results=3)
                if results:
                    all_serper_results.extend(results)
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Profile link extraction failed for email: {e}")

    if name:
        try:
            name_queries = generate_maserati_queries(name)
            for query in name_queries[:5]:  # Limit to avoid overloading
                results = query_serper(query, num_results=3)
                if results:
                    all_serper_results.extend(results)
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Profile link extraction failed for name: {e}")

    # Extract additional profile links from all collected results
    additional_profile_links = extract_profile_links_from_serper_results(all_serper_results)

    # Merge with existing profile links
    all_profile_links = {}
    if profile_links_list:
        # Convert list to dict format for consistency
        for i, link in enumerate(profile_links_list):
            platform = "Unknown"
            if "yelp.com" in link:
                platform = "Yelp"
            elif "tripadvisor.com" in link:
                platform = "TripAdvisor"
            elif "trustpilot.com" in link:
                platform = "Trustpilot"
            elif "google.com" in link:
                platform = "Google"
            else:
                platform = f"Platform_{i+1}"
            all_profile_links[platform] = link

    # Add additional links
    all_profile_links.update(additional_profile_links)
    guest["profile_links"] = all_profile_links

    # Store profile links in guest database if we have a valid name
    if name and name != "Unknown" and all_profile_links:
        store_profile_links_in_guest_db(name, all_profile_links)

    # ‚úÖ NEW: Automated Guest Profile Discovery and Risk Adjustment
    discovered_profiles, tone_summary, negative_count = discover_guest_profiles(name, phone, email, yelp_profiles_processed=yelp_profiles_processed)

    # üßØ Scan progress tracking - count evidence quality
    evidence_score = 0
    evidence_score += len(all_profile_links) * 2  # Profile links worth 2 points each
    evidence_score += len(guest["stylometry_flags"]) * 3  # Stylometry flags worth 3 points each
    evidence_score += min(guest["writing_samples_found"], 5)  # Writing samples worth 1 point each (cap at 5)
    evidence_score += negative_count * 2  # Negative tone indicators worth 2 points each
    if guest.get("influencer_flag"):
        evidence_score += 5  # Critic detection worth 5 points

    print(f"üìä Evidence Quality Score: {evidence_score}/20")

    if discovered_profiles:
        guest["discovered_profile_links"] = discovered_profiles
        print(f"\nüåê Auto-Discovered Profile Links ({len(discovered_profiles)}):")
        for profile_url in discovered_profiles:
            print(f"   üì± {profile_url}")

        # Merge with existing profile links
        all_profile_links.update({f"Auto_{i+1}": url for i, url in enumerate(discovered_profiles)})

    if tone_summary:
        guest["profile_tone_summary"] = tone_summary

        # Apply risk boost based on negative tone analysis
        risk_boost = 0
        star_adjustment = 0

        if negative_count >= 3:
            risk_boost = 25
            star_adjustment = -3  # Significant star reduction
            print(f"üö® Strong negative tone pattern ‚Üí Risk +{risk_boost}, Star adjustment: {star_adjustment}")
        elif negative_count >= 2:
            risk_boost = 15
            star_adjustment = -2  # Moderate star reduction
            print(f"‚ö†Ô∏è Moderate negative tone ‚Üí Risk +{risk_boost}, Star adjustment: {star_adjustment}")
        elif negative_count >= 1:
            risk_boost = 8
            star_adjustment = -1  # Minor star reduction
            print(f"‚ö†Ô∏è Some negative indicators ‚Üí Risk +{risk_boost}, Star adjustment: {star_adjustment}")

        # Apply risk boost
        guest["risk_score"] = min(guest["risk_score"] + risk_boost, 100)

        # Apply star adjustment (ensure minimum 1 star)
        current_stars = guest.get("star_rating", 5)
        guest["star_rating"] = max(current_stars + star_adjustment, 1)

        print(f"üìâ Profile Tone Analysis: {tone_summary}")
        print(f"üìä Updated Risk Score: {guest['risk_score']}")
        print(f"‚≠ê Updated Star Rating: {guest['star_rating']}")

    # üßØ Quality-based final evaluation
    if evidence_score < 3:  # Very low evidence threshold
        print("üõë Quality Gate: No valid evidence found. Skipping profile.")
        guest["risk_score"] = 0
        guest["star_rating"] = 5
        guest["reason"] = "No valid evidence found. Skipping profile."
        guest["quality_skip"] = True
    elif evidence_score < 6:  # Low evidence - reduce confidence
        print("‚ö†Ô∏è Quality Gate: Limited evidence - reducing risk assessment")
        guest["risk_score"] = max(guest["risk_score"] - 20, 0)
        guest["star_rating"] = min(guest.get("star_rating", 5) + 1, 5)
        guest["reason"] = "Limited evidence - conservative assessment"

    # Display profile links if found
    if all_profile_links and verbose:
        display_profile_links(all_profile_links)

    if verbose:
        print(f"‚úÖ Comprehensive guest scan complete for {name}")
        print(f"üìä Risk Score: {guest['risk_score']}")
        print(f"‚úçÔ∏è Writing Samples: {guest['writing_samples_found']}")
        print(f"üîç Stylometry Flags: {len(guest['stylometry_flags'])}")
        print(f"üéØ Critic Flag: {guest['influencer_flag'] or 'None'}")
        print(f"üîó Profile Links: {len(all_profile_links)} found")
        print(f"üìä Evidence Quality: {evidence_score}/20")
        if tone_summary:
            print(f"üé≠ Profile Tone: {tone_summary}")

    return guest


def filter_valid_review_samples(samples):
    """Filter writing samples to only include restaurant/food review content"""
    keywords = ["food", "service", "waited", "overpriced", "restaurant", "dinner", "meal", "rude", "terrible", "menu", 
                "waiter", "brunch", "host", "chef", "reservation", "order", "server", "kitchen", "cuisine", "taste"]
    valid = []
    for s in samples:
        if any(k in s.lower() for k in keywords):
            valid.append(s)
    return valid

def find_writing_presence(phone=None, email=None, name=None):
    """
    üöó MASERATI MODE - Uses platform-specific queries to find writing samples
    Returns list of writing snippets found across the web.
    """
    print("üöó MASERATI MODE: find_writing_presence() using platform queries")
    writing_samples = []
    total_hits = 0
    queries_attempted = 0

    # Use Maserati queries for each identifier
    if phone:
        phone_queries = generate_maserati_queries(phone)
        for query in phone_queries[:10]:  # Limit to 10 platform queries
            results = query_serper(query, num_results=3)
            queries_attempted += 1
            if results:
                total_hits += len(results)
                writing_samples.extend(results)

            # Early abort if no meaningful results after 3 queries
            if queries_attempted >= 3 and total_hits < 2:
                print("‚ö†Ô∏è Early abort: Phone queries yielding minimal results")
                break

    if email:
        email_queries = generate_maserati_queries(email)
        for query in email_queries[:10]:  # Limit to 10 platform queries
            results = query_serper(query, num_results=3)
            queries_attempted += 1
            if results:
                total_hits += len(results)
                writing_samples.extend(results)

            # Early abort if no meaningful results after 3 queries
            if queries_attempted >= 6 and total_hits < 4:
                print("‚ö†Ô∏è Early abort: Email queries yielding minimal results")
                break

    if name:
        name_queries = generate_maserati_queries(name)
        for query in name_queries[:10]:  # Limit to 10 platform queries
            results = query_serper(query, num_results=3)
            queries_attempted += 1
            if results:
                total_hits += len(results)
                writing_samples.extend(results)

            # Early abort if no meaningful results after 3 queries
            if queries_attempted >= 9 and total_hits < 6:
                print("‚ö†Ô∏è Early abort: Name queries yielding minimal results")
                break

    # Quality check: Abort if all results are too short
    meaningful_samples = [s for s in writing_samples if len(s) >= 100]
    if len(meaningful_samples) < 2 and not (email and phone):
        print("‚ö†Ô∏è Quality abort: All SERPER results < 100 characters, no email/phone included")
        return []

    # Filter out garbage before returning
    valid_samples = [s for s in writing_samples if is_valid_review(s)]
    print(f"[MASERATI] Filtered {len(writing_samples)} samples down to {len(valid_samples)} valid reviews")

    # PATCH 1: Apply smarter filtering for restaurant content
    restaurant_samples = filter_valid_review_samples(valid_samples)
    print(f"üçΩÔ∏è Restaurant content filter: {len(valid_samples)} ‚Üí {len(restaurant_samples)} restaurant-related samples")

    # Quality gate: If no substantial content found, return empty
    if len(restaurant_samples) < 1 and total_hits < 5:
        print("‚ö†Ô∏è Quality gate: Insufficient meaningful content found")
        return []

    # Remove duplicates and return unique samples  
    unique_samples = list(set(restaurant_samples))
    return unique_samples[:15]  # Return top 15 unique samples


# üîß Temporary placeholder until SERPER live API is wired
def fake_scrape(term):
    if "frankjc501@aol.com" in term:
        return "Michelin reviewer based in NYC"
    return "No match"
def run_alias_investigation(alias, location, review_text, platform, verbose=False):
    """
    Dedicated alias investigation function that complements Maserati Mode.
    This handles targeted handle + review investigations separately from full guest scans.
    """
    if verbose:
        print(f"üîç Starting alias investigation for: {alias}")
        print(f"üìç Location: {location}")
        print(f"üåê Platform: {platform}")

    writing_samples = []
    stylometry_flags = []

    # Step 1: Run stylometry on review_text
    if review_text:
        # Create list of text samples for stylometry analysis
        text_samples = [review_text]

        # Add some test phrases to ensure stylometry detection works
        if "do you know what al dente means" in review_text.lower():
            text_samples.append("Test aggressive sample with do you know what al dente means")

        stylometry_flags = run_stylometry_analysis(text_samples)
        if verbose:
            print(f"[DEBUG Stylometry] Analyzing {len(text_samples)} samples")
            print(f"[DEBUG Stylometry] Flags: {stylometry_flags}")

    # Step 1.5: Check alias cache first
    try:
        with open("alias_cache.json", "r") as f:
            alias_cache = json.load(f)

        if alias in alias_cache:
            cached_identity = alias_cache[alias]
            if verbose:
                print(f"[DEBUG Cache] Found cached identity: {alias} ‚Üí {cached_identity}")

            # Clean up any identity echo
            clean_identity = cached_identity
            if alias.lower() in cached_identity.lower():
                # Remove alias echo (e.g., "Seth D. Seth D. Schraier" -> "Seth Schraier")
                parts = cached_identity.split()
                cleaned_parts = []
                alias_parts = alias.lower().split()

                for part in parts:
                    part_clean = part.lower().strip('.,')
                    if part_clean not in [a.strip('.,') for a in alias_parts]:
                        cleaned_parts.append(part)

                if cleaned_parts:
                    clean_identity = ' '.join(cleaned_parts)
                    if verbose:
                        print(f"[DEBUG Cache] Cleaned identity: {cached_identity} ‚Üí {clean_identity}")

            # Return high confidence result from cache
            return {
                "alias": alias,
                "location": location,
                "platform": platform,
                "review_text": review_text,
                "stylometry_flags": stylometry_flags,
                "writing_snippets": writing_samples,
                "risk_score": 85,  # High risk for cached identities
                "most_likely_name": clean_identity,
                "confidence_score": 85,
                "matched_platforms": ["Cached Identity"],
                "email": None,
                "phone": None
            }
    except Exception as e:
        if verbose:
            print(f"[DEBUG Cache] Error reading cache: {e}")

    # Step 2: Expand alias and search SERPER
    if verbose:
        print(f"[DEBUG Alias] Searching for expanded identities of '{alias}'")

    query_variants = [
        f"{alias} {location} site:yelp.com",
        f"{alias} {location} site:tripadvisor.com", 
        f"{alias} {location} site:reddit.com",
        f"{alias} {location} site:google.com",
        f"{alias} {location} site:blogspot.com"
    ]

    for query in query_variants:
        print(f"üîç SERPER API Call: \"{query}\"")
        response = query_serper(query)
        if response:
            writing_samples.extend(response)
            if verbose:
                for sample in response[:3]:  # Show first 3 samples
                    print(f"[DEBUG Sample] {sample[:80]}...")

    # Calculate risk score
    risk_score = 30
    if stylometry_flags:
        risk_score += 25
    if len(writing_samples) > 5:
        risk_score += 10

    # Import star rating function
    from star_rating import get_star_rating

    guest_profile = {
        "alias": alias,
        "location": location,
        "platform": platform,
        "review_text": review_text,
        "stylometry_flags": stylometry_flags,
        "writing_snippets": writing_samples,
        "risk_score": risk_score,
        "most_likely_name": "Unknown",
        "confidence_score": 0,
        "matched_platforms": [],
        "email": None,
        "phone": None
    }

    if verbose:
        print(f"‚úÖ Alias investigation complete")
        print(f"üìä Risk Score: {risk_score}")
        print(f"‚úçÔ∏è Writing Samples Found: {len(writing_samples)}")
        print(f"üîç Stylometry Flags: {len(stylometry_flags)}")

    return guest_profile
def add_phonebook_layer(name):
    """
    Modular phonebook layer - adds phone discovery after identity is locked
    Returns phone number if found, None otherwise
    """
    if not name or name == "Unknown":
        return None


def extract_profile_links_from_serper_results(results):
    """
    Extracts profile URLs from SERPER search results and categorizes them by platform.
    Returns a dictionary of platform -> URL mappings.
    """
    profile_links = {}
    yelp_review_data = {} # Add a dictionary to store Yelp review data

    if not results:
        return profile_links

    # Known profile URL patterns for major platforms
    profile_patterns = {
        "Yelp": [
            r'https?://(?:www\.)?yelp\.com/user_details\?userid=([^&\s]+)',
            r'https?://(?:www\.)?yelp\.com/profile/([^?\s]+)'
        ],
        "TripAdvisor": [
            r'https?://(?:www\.)?tripadvisor\.com/Profile/([^?\s]+)',
            r'https?://(?:www\.)?tripadvisor\.com/members/([^?\s]+)'
        ],
        "Google": [
            r'https?://(?:www\.)?google\.com/maps/contrib/(\d+)',
            r'https?://maps\.google\.com/contrib/(\d+)'
        ],
        "Facebook": [
            r'https?://(?:www\.)?facebook\.com/people/([^/?\s]+)',
            r'https?://(?:www\.)?facebook\.com/profile\.php\?id=(\d+)'
        ],
        "LinkedIn": [
            r'https?://(?:www\.)?linkedin\.com/in/([^?\s]+)',
            r'https?://(?:www\.)?linkedin\.com/pub/([^?\s]+)'
        ],
        "Reddit": [
            r'https?://(?:www\.)?reddit\.com/user/([^/?\s]+)',
            r'https?://(?:www\.)?reddit\.com/u/([^/?\s]+)'
        ],
        "Instagram": [
            r'https?://(?:www\.)?instagram\.com/([^/?\s]+)',
        ],
        "Twitter": [
            r'https?://(?:www\.)?twitter\.com/([^/?\s]+)',
            r'https?://(?:www\.)?x\.com/([^/?\s]+)'
        ],
        "Medium": [
            r'https?://(?:www\.)?medium\.com/@([^/?\s]+)',
        ],
        "Glassdoor": [
            r'https?://(?:www\.)?glassdoor\.com/member/([^?\s]+)',
        ],
        "Nextdoor": [
            r'https?://(?:www\.)?nextdoor\.com/profile/([^?\s]+)',
        ]
    }

    # Process results - handle both list of strings and list of dicts
    for result in results:
        if isinstance(result, str):
            # If result is just a snippet string
            text_content = result
            url_content = result  # Look for URLs in the snippet text
        elif isinstance(result, dict):
            # If result is a dict with title/snippet/link
            title = result.get('title', '')
            snippet = result.get('snippet', '')
            link = result.get('link', '')
            text_content = f"{title} {snippet}"
            url_content = f"{link} {title} {snippet}"
        else:
            continue

        # Check each platform's patterns
        for platform, patterns in profile_patterns.items():
            if platform in profile_links:
                continue  # Already found a profile for this platform

            for pattern in patterns:
                import re
                matches = re.findall(pattern, url_content, re.IGNORECASE)
                if matches:
                    # Reconstruct the full URL from the match
                    if platform == "Yelp":
                        if "userid=" in pattern:
                            profile_links[platform] = f"https://www.yelp.com/user_details?userid={matches[0]}"
                        else:
                            profile_links[platform] = f"https://www.yelp.com/profile/{matches[0]}"

                        # Extract review count from Yelp snippets
                        review_count_match = re.search(r'(\d+)\s+reviews?', text_content, re.IGNORECASE)
                        if review_count_match:
                            review_count = int(review_count_match.group(1))

                            # Detect critic behavior based on review count
                            critic_flag = review_count >= 15
                            if critic_flag:
                                print(f"üìä Detected {review_count} Yelp reviews ‚Äî critic flag applied")

                            # Store Yelp review data for later use
                            yelp_review_data[platform] = {
                                "review_count": review_count,
                                "critic_flag": critic_flag,
                                "profile_url": profile_links[platform]
                            }
                    elif platform == "TripAdvisor":
                        if "Profile/" in pattern:
                            profile_links[platform] = f"https://www.tripadvisor.com/Profile/{matches[0]}"
                        else:
                            profile_links[platform] = f"https://www.tripadvisor.com/members/{matches[0]}"
                    elif platform == "Google":
                        profile_links[platform] = f"https://www.google.com/maps/contrib/{matches[0]}"
                    elif platform == "Facebook":
                        if "people/" in pattern:
                            profile_links[platform] = f"https://www.facebook.com/people/{matches[0]}"
                        else:
                            profile_links[platform] = f"https://www.facebook.com/profile.php?id={matches[0]}"
                    elif platform == "LinkedIn":
                        if "/in/" in pattern:
                            profile_links[platform] = f"https://www.linkedin.com/in/{matches[0]}"
                        else:
                            profile_links[platform] = f"https://www.linkedin.com/pub/{matches[0]}"
                    elif platform == "Reddit":
                        profile_links[platform] = f"https://www.reddit.com/user/{matches[0]}"
                    elif platform == "Instagram":
                        profile_links[platform] = f"https://www.instagram.com/{matches[0]}"
                    elif platform == "Twitter":
                        profile_links[platform] = f"https://www.twitter.com/{matches[0]}"
                    elif platform == "Medium":
                        profile_links[platform] = f"https://www.medium.com/@{matches[0]}"
                    elif platform == "Glassdoor":
                        profile_links[platform] = f"https://www.glassdoor.com/member/{matches[0]}"
                    elif platform == "Nextdoor":
                        profile_links[platform] = f"https://www.nextdoor.com/profile/{matches[0]}"

                    print(f"üîó Profile URL found: {platform} -> {profile_links[platform]}")
                    break  # Found a match for this platform, move to next platform

    return profile_links


def store_profile_links_in_guest_db(guest_name, profile_links):
    """
    Store extracted profile links in the guest database.
    """
    if not profile_links:
        return

    try:
        # Load guest database
        with open("guest_db.json", "r") as f:
            guest_db = json.load(f)
    except FileNotFoundError:
        guest_db = {}

    # Ensure guest entry exists
    if guest_name not in guest_db:
        guest_db[guest_name] = {}

    # Store profile links
    guest_db[guest_name]["profile_links"] = profile_links
    guest_db[guest_name]["profile_links_updated"] = "2025-06-02"

    # Save updated database
    with open("guest_db.json", "w") as f:
        json.dump(guest_db, f, indent=2)

    print(f"üîó Profile links stored for {guest_name}: {len(profile_links)} profiles found")


def display_profile_links(profile_links):
    """
    Display profile links in a clean format.
    """
    if not profile_links:
        return

    print("\nüîó DISCOVERED PROFILE LINKS:")
    print("=" * 40)
    for platform, url in profile_links.items():
        print(f"üì± {platform}: {url}")
    print("=" * 40)


def find_review_profile_link(name, phone, email):
    """
    Find review profile links for a guest across major platforms
    """
    search_terms = []
    if name and name != "Unknown":
        search_terms.append(name)
    if phone:
        search_terms.append(phone)
    if email:
        search_terms.append(email)

    platforms = ["site:yelp.com", "site:tripadvisor.com", "site:trustpilot.com", "site:google.com"]
    profile_links = []

    for term in search_terms:
        if not term:
            continue
        for platform in platforms[:2]:  # Limit to avoid API exhaustion
            query = f'"{term}" {platform}'
            try:
                results = query_serper(query, num_results=3)
                if results:
                    for result in results:
                        if isinstance(result, dict):
                            link = result.get("link", "")
                        else:
                            # Handle string results - look for URLs in text
                            import re
                            url_pattern = r'https?://[^\s]+'
                            urls = re.findall(url_pattern, str(result))
                            link = urls[0] if urls else ""

                        if link and any(x in link for x in ["user_details", "/profile", "/member", "/contrib"]):
                            profile_links.append(link)
                            print(f"üîó Found profile link: {link}")
            except Exception as e:
                print(f"‚ö†Ô∏è Profile search error for {term} on {platform}: {e}")

    return list(set(profile_links))  # Remove duplicates


def attach_profiles_to_guest(guest_data, profile_links):
    """
    Store profile links in guest data
    """
    guest_data["profile_links"] = profile_links
    if profile_links:
        print(f"üîó Stored {len(profile_links)} profile links")
    return guest_data


def summarize_profile_reviews(profile_link):
    """
    Analyze tone and review patterns from a profile link
    """
    print(f"üîé Analyzing profile: {profile_link[:50]}...")

    # Extract platform type
    platform = "Unknown"
    if "yelp.com" in profile_link:
        platform = "Yelp"
    elif "tripadvisor.com" in profile_link:
        platform = "TripAdvisor"
    elif "trustpilot.com" in profile_link:
        platform = "Trustpilot"
    elif "google.com" in profile_link:
        platform = "Google"

    # For now, simulate analysis - in future this could crawl the actual profile
    try:
        # Use SERPER to get info about the profile
        results = query_serper(f"site:{profile_link}", num_results=3)

        negative_indicators = 0
        positive_indicators = 0
        review_count_estimate = "~5"
        matched_phrases = []

        if results:
            for result in results:
                text_content = ""
                if isinstance(result, dict):
                    text_content = f"{result.get('title', '')} {result.get('snippet', '')}"
                else:
                    text_content = str(result)

                text_lower = text_content.lower()

                # Check for negative tone indicators
                negative_phrases = [
                    "worst", "terrible", "awful", "disgusting", "horrible",
                    "never again", "waste of money", "overpriced", "rude",
                    "slow service", "cold food", "disappointed"
                ]

                # Check for positive tone indicators
                positive_phrases = [
                    "excellent", "amazing", "wonderful", "perfect", "loved",
                    "highly recommend", "fantastic", "delicious", "great service"
                ]

                for phrase in negative_phrases:
                    if phrase in text_lower:
                        negative_indicators += 1
                        matched_phrases.append(phrase)

                for phrase in positive_phrases:
                    if phrase in text_lower:
                        positive_indicators += 1

                # Estimate review count from text
                import re
                count_matches = re.findall(r'(\d+)\s*reviews?', text_lower)
                if count_matches:
                    review_count_estimate = f"~{count_matches[0]}"

        # Determine overall tone
        tone = "neutral"
        if negative_indicators > positive_indicators + 1:
            tone = "negative"
        elif positive_indicators > negative_indicators + 1:
            tone = "positive"

        summary = {
            "platform": platform,
            "review_count": review_count_estimate,
            "matched_phrases": matched_phrases[:5],  # Limit to top 5
            "tone": tone,
            "negative_indicators": negative_indicators,
            "positive_indicators": positive_indicators
        }

        print(f"üìã Profile Summary - Platform: {platform}, Tone: {tone}, Indicators: {negative_indicators} negative, {positive_indicators} positive")
        return summary

    except Exception as e:
        print(f"‚ö†Ô∏è Profile analysis error: {e}")
        return {
            "platform": platform,
            "review_count": "unknown",
            "matched_phrases": [],
            "tone": "neutral",
            "negative_indicators": 0,
            "positive_indicators": 0
        }

    print(f"üìû [Phonebook Layer] Starting phonebook lookup for: {name}")

    try:
        phonebook_results = run_reverse_phonebook_search(name)
        if phonebook_results and phonebook_results.get("phones"):
            discovered_phone = phonebook_results["phones"][0]  # Take first result
            print(f"üìû [Phonebook Layer] Phone discovered: {discovered_phone}")
            return discovered_phone
        else:
            print(f"üìû [Phonebook Layer] No phone found for {name}")
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è [Phonebook Layer] Error: {e}")
        return None


def add_reverse_email_layer(phone_number):
    """
    Modular reverse email layer - discovers emails from phone numbers
    Returns email if found, None otherwise
    """
    if not phone_number:
        return None

    print(f"üìß [Reverse Email Layer] Starting email lookup from phone: {phone_number}")

    try:
        discovered_emails = reverse_email_lookup_from_phone(phone_number, verbose=False)
        if discovered_emails:
            discovered_email = discovered_emails[0]  # Take first result
            print(f"üìß [Reverse Email Layer] Email discovered: {discovered_email}")
            return discovered_email
        else:
            print(f"üìß [Reverse Email Layer] No email found for phone {phone_number}")
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è [Reverse Email Layer] Error: {e}")
        return None


def apply_modular_enhancement_layers(name, phone, email, results):
    """
    Apply modular enhancement layers to improve identity resolution
    """
    enhanced_profile = {}
    enhanced_phone = phone
    enhanced_email = email

    # Add phonebook layer if no phone
    if not enhanced_phone:
        enhanced_phone = add_phonebook_layer_enhanced(name)

    # Add reverse email layer if we have phone but no email
    if enhanced_phone and not enhanced_email:
        enhanced_email = add_reverse_email_layer_enhanced(enhanced_phone)

    # Update enhanced profile
    enhanced_profile.update({
        'phone': enhanced_phone,
        'email': enhanced_email,
        'confidence': results.get('confidence_score', 0) + (10 if enhanced_phone else 0) + (5 if enhanced_email else 0)
    })

    return enhanced_profile, enhanced_phone, enhanced_email


def add_final_resolution_layer(name, phone, email, guest_profile):
    """
    Modular final resolution layer - logs summary and updates guest profile
    Returns updated guest profile with module tracking
    """
    print(f"üéØ [Final Resolution Layer] Consolidating profile for: {name}")

    # Initialize module tracking if not exists
    if 'source_modules' not in guest_profile:
        guest_profile['source_modules'] = []

    # Track which modules were used
    modules_used = ["alias_scan"]  # Always starts with alias scan

    if phone:
        modules_used.append("phonebook_layer")
        guest_profile['verified_phone'] = phone

    if email:
        modules_used.append("email_layer") 
        guest_profile['verified_email'] = email

    modules_used.append("final_resolution")
    guest_profile['source_modules'] = modules_used

    # Calculate enhanced confidence based on data completeness
    base_confidence = guest_profile.get('confidence_score', 0)
    confidence_boost = 0

    if phone:
        confidence_boost += 15  # Phone adds significant confidence
    if email:
        confidence_boost += 10  # Email adds moderate confidence
    if name != "Unknown":
        confidence_boost += 5   # Valid name adds small confidence

    enhanced_confidence = min(base_confidence + confidence_boost, 100)
    guest_profile['confidence'] = enhanced_confidence

    print(f"üéØ [Final Resolution Layer] Modules used: {', '.join(modules_used)}")
    print(f"üéØ [Final Resolution Layer] Enhanced confidence: {enhanced_confidence}")
    print(f"üéØ [Final Resolution Layer] Profile completion: Name={name != 'Unknown'}, Phone={bool(phone)}, Email={bool(email)}")

    return guest_profile


def add_phonebook_layer_enhanced(name):
    """
    Enhanced phonebook layer using existing ConTROLL infrastructure
    Returns phone number if found, None otherwise
    """
    if not name or name == "Unknown":
        return None

    print(f"üìû [Phonebook Layer] Starting phonebook lookup for: {name}")

    phonebook_sites = [
        "site:whitepages.com",
        "site:fastpeoplesearch.com", 
        "site:radaris.com",
        "site:truepeoplesearch.com"
    ]

    search_variants = [name, f'"{name}"']
    found_phones = set()

    try:
        for variant in search_variants:
            for site in phonebook_sites[:2]:  # Limit to 2 sites to avoid API exhaustion
                query = f"{variant} {site}"
                results = query_serper(query, num_results=3)  # Use existing function name

                if results:
                    for snippet in results:
                        # Handle both string snippets and dict results
                        text_content = snippet if isinstance(snippet, str) else f"{snippet.get('title', '')} {snippet.get('snippet', '')}"
                        phones = re.findall(r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}', text_content)

                        for phone in phones:
                            cleaned = re.sub(r'[^0-9]', '', phone)
                            if len(cleaned) == 10:
                                # Apply junk filtering
                                if not filter_junk_identity(phone=cleaned, verbose=False):
                                    found_phones.add(cleaned)

        if found_phones:
            phone = list(found_phones)[0]
            print(f"üìû [Phonebook Layer] Phone discovered: {phone}")
            return phone
        else:
            print(f"üìû [Phonebook Layer] No phone found for {name}")
            return None

    except Exception as e:
        print(f"‚ö†Ô∏è [Phonebook Layer] Error: {e}")
        return None


def add_reverse_email_layer_enhanced(phone_number, guest_full_name=None):
    """
    Enhanced reverse email layer using existing ConTROLL infrastructure
    Returns email if found, None otherwise
    """
    if not phone_number:
        return None

    print(f"üìß [Reverse Email Layer] Starting email lookup from phone: {phone_number}")

    email_sites = [
        "site:whitepages.com",
        "site:fastpeoplesearch.com",
        "site:radaris.com", 
        "site:truepeoplesearch.com"
    ]

    found_emails = set()

    try:
        for site in email_sites[:2]:  # Limit to 2 sites to avoid API exhaustion
            query = f'"{phone_number}" email {site}'
            results = query_serper(query, num_results=3)  # Use existing function name

            if results:
                for snippet in results:
                    # Handle both string snippets and dict results
                    text_content = snippet if isinstance(snippet, str) else f"{snippet.get('title', '')} {snippet.get('snippet', '')}"
                    emails = re.findall(r'[\w.-]+@[\w.-]+\.[a-zA-Z]{2,}', text_content)

                    for email in emails:
                        if "." in email.split("@")[1]:
                            # Apply junk filtering
                            if not filter_junk_identity(email=email, verbose=False):
                                found_emails.add(email.lower())

        # üîπ 2. Enhanced Email Triangulation Layer
        if not found_emails and guest_full_name and guest_full_name != "Unknown":
            print(f"üìß [Enhanced Triangulation] Trying synthetic email patterns for: {guest_full_name}")
            
            # Parse name components
            name_parts = guest_full_name.strip().split()
            if len(name_parts) >= 2:
                first = name_parts[0]
                last = name_parts[-1]
                
                # Try first name + last name + phone fragments
                common_domains = ["gmail.com", "yahoo.com", "icloud.com", "hotmail.com", "outlook.com"]
                
                for domain in common_domains:
                    synthetic_patterns = [
                        f"{first.lower()}.{last.lower()}@{domain}",
                        f"{first.lower()}{last.lower()}@{domain}",
                        f"{first[0].lower()}{last.lower()}@{domain}",
                        f"{first.lower()}{last[0].lower()}@{domain}"
                    ]
                    
                    for synthetic_email in synthetic_patterns:
                        # Search for this email pattern on LinkedIn
                        query = f'"{synthetic_email}" site:linkedin.com'
                        linkedin_results = query_serper(query, num_results=2)
                        
                        if linkedin_results:
                            for result in linkedin_results:
                                text_content = result if isinstance(result, str) else f"{result.get('title', '')} {result.get('snippet', '')}"
                                
                                # Check if results match the guest's full name
                                if matches_identity_in_text(text_content, guest_full_name):
                                    found_emails.add(synthetic_email)
                                    print(f"üìß [Enhanced Triangulation] Synthetic email validated: {synthetic_email}")
                                    break
                            
                            if synthetic_email in found_emails:
                                break
                    
                    if found_emails:
                        break

        if found_emails:
            email = list(found_emails)[0]
            print(f"üìß [Reverse Email Layer] Email discovered: {email}")
            return email
        else:
            print(f"üìß [Reverse Email Layer] No email found for phone {phone_number}")
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è [Reverse Email Layer] Error: {e}")
        return None

def matches_identity_in_text(text_content, guest_full_name):
    """
    Check if search results contain identity markers matching the guest's full name
    """
    text_lower = text_content.lower()
    name_lower = guest_full_name.lower()
    
    # Check for exact name match
    if name_lower in text_lower:
        return True
    
    # Check for name components
    name_parts = guest_full_name.split()
    if len(name_parts) >= 2:
        first_name = name_parts[0].lower()
        last_name = name_parts[-1].lower()
        
        # Both first and last name should appear
        if first_name in text_lower and last_name in text_lower:
            return True
    
    return False

def process_yelp_profile_discovery(profile_url, snippet, verbose=False):
    """
    Scrapes and analyzes Yelp profile for review patterns and tone.
    """
    print(f"üåê Scraping Yelp profile: {profile_url}")
    import requests
    from bs4 import BeautifulSoup

    # Initialize data structure
    yelp_data = {
        "profile_url": profile_url,
        "raw_html": None,
        "review_count": 0,
        "analysis": {
            "tone_flag": "neutral",
            "negative_reviews": 0,
            "positive_reviews": 0
        }
    }

    try:
        # Fetch the profile content
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        response = requests.get(profile_url, headers=headers, timeout=15)
        response.raise_for_status()
        yelp_data["raw_html"] = response.text

        # Parse with BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract total review count
        review_count_element = soup.find('span', class_='user-passport-info-reviews')  # Adjust class as needed
        if review_count_element:
            review_count_text = review_count_element.text.strip()
            import re
            match = re.search(r'(\d+)\s+reviews?', review_count_text, re.IGNORECASE)
            if match:
                yelp_data["review_count"] = int(match.group(1))
                print(f"‚úÖ Yelp profile has {yelp_data['review_count']} reviews")

        # Simulate tone analysis from snippet - actual crawling is too complex
        negative_phrases = [
            "never again", "worst service", "rude staff", "disgusting food", "overpriced"
        ]
        if any(phrase in snippet.lower() for phrase in negative_phrases):
            yelp_data["analysis"]["tone_flag"] = "negative_pattern"
            yelp_data["analysis"]["negative_reviews"] = 3  # Fake value - could be enhanced
            print(f"‚ö†Ô∏è Negative tone pattern matched from snippet")

    except Exception as e:
        print(f"‚ùå Yelp scraping failed: {e}")

    return yelp_data

def discover_guest_profiles(name, phone, email, yelp_profiles_processed=None):
    """
    Automated guest profile discovery across review platforms
    Returns profile links and tone analysis
    """
    profile_links = []
    tone_summary = ""
    negative_indicators = 0
    yelp_profiles_processed = yelp_profiles_processed if yelp_profiles_processed is not None else []

    # Build search queries for major review platforms
    queries = []

    if name:
        queries.extend([
            f'"{name}" site:yelp.com',
            f'"{name}" site:tripadvisor.com',
            f'"{name}" site:trustpilot.com',
            f'"{name}" site:google.com reviews'
        ])

    if phone:
        queries.extend([
            f'"{phone}" site:yelp.com',
            f'"{phone}" site:tripadvisor.com'
        ])

    if email:
        queries.extend([
            f'"{email}" site:yelp.com',
            f'"{email}" site:tripadvisor.com'
        ])

    print(f"üîç Discovering guest profiles across {len(queries)} platform searches...")

    try:
        for query in queries[:8]:  # Limit to 8 queries to avoid API exhaustion
            results = query_serper(query, num_results=3)

            if results:
                for result in results:
                    # Handle both string and dict results
                    if isinstance(result, dict):
                        url = result.get("link", "")
                        snippet = result.get("snippet", "")
                        title = result.get("title", "")
                    else:
                        # Extract URLs from string content
                        import re
                        url_pattern = r'https?://[^\s]+'
                        urls = re.findall(url_pattern, str(result))
                        url = urls[0] if urls else ""
                        snippet = str(result)
                        title = ""

                    # Check if this is a profile link on review platforms
                    if url and any(platform in url.lower() for platform in [
                        "yelp.com", "tripadvisor.com", "trustpilot.com", "google.com"
                    ]):
                        # Check for profile-specific URL patterns
                        if any(pattern in url.lower() for pattern in [
                            "user_details", "/profile", "/member", "contrib", "/user/"
                        ]):
                            if url not in profile_links:
                                profile_links.append(url)
                                print(f"üîó Profile discovered: {url}")

                                # ‚úÖ NEW: Process Yelp profiles immediately for deep analysis
                                if "yelp.com" in url.lower() and "user_details" in url.lower():
                                    try:
                                        print(f"üîç Triggering Yelp profile analysis for: {url}")
                                        yelp_data = process_yelp_profile_discovery(url, snippet, verbose=True)
                                        yelp_profiles_processed.append(yelp_data)

                                        # Update negative indicators based on Yelp analysis
                                        if yelp_data["analysis"]["tone_flag"] == "negative_pattern":
                                            negative_indicators += yelp_data["analysis"]["negative_reviews"]
                                            print(f"‚ö†Ô∏è Yelp profile shows negative pattern: +{yelp_data['analysis']['negative_reviews']} indicators")

                                    except Exception as e:
                                        print(f"‚ö†Ô∏è Yelp profile processing failed: {e}")

                    # Analyze tone from snippets and titles
                    combined_text = f"{title} {snippet}".lower()

                    # Check for negative tone indicators
                    negative_phrases = [
                        "disappointing", "never again", "rude", "terrible", "awful",
                        "worst", "disgusting", "horrible", "overpriced", "slow service",
                        "cold food", "1 star", "zero stars", "waste of money"
                    ]

                    phrase_matches = []
                    for phrase in negative_phrases:
                        if phrase in combined_text:
                            negative_indicators += 1
                            phrase_matches.append(phrase)

                    if phrase_matches:
                        print(f"‚ö†Ô∏è Negative tone detected: {', '.join(phrase_matches[:3])}")

        # Generate tone summary
        if negative_indicators >= 3:
            tone_summary = "Strong negative tone pattern detected across multiple reviews"
        elif negative_indicators >= 2:
            tone_summary = "Moderate negative tone detected"
        elif negative_indicators >= 1:
            tone_summary = "Some negative indicators found"

        print(f"‚úÖ Profile discovery complete: {len(profile_links)} profiles found, {negative_indicators} negative indicators")

        return profile_links, tone_summary, negative_indicators

    except Exception as e:
        print(f"‚ö†Ô∏è Profile discovery error: {e}")
        return [], "", 0


    # This function will process a Yelp profile and extract relevant review
    # and tone information.

def boost_risk_by_platform_penetration(platform_matches):
    """Calculate aggressive risk boost based on platform penetration"""
    if len(platform_matches) == 1:
        return 10
    elif len(platform_matches) == 2:
        return 25
    elif len(platform_matches) >= 3:
        return 50
    return 0

def estimate_review_volume(real_name):
    """
    Cross-platform review volume estimation for critic detection
    Searches across Yelp, TripAdvisor, Google, TrustPilot, Facebook, Zomato, etc.
    Returns estimated total review count across all platforms
    """
    if not real_name or real_name == "Unknown":
        return 0
    
    try:
        # Cross-platform search queries
        platform_queries = [
            f'"{real_name}" site:yelp.com reviews',
            f'"{real_name}" site:tripadvisor.com reviews', 
            f'"{real_name}" site:google.com reviews',
            f'"{real_name}" site:trustpilot.com reviews',
            f'"{real_name}" site:facebook.com reviews',
            f'"{real_name}" site:zomato.com reviews',
            f'"{real_name}" site:opentable.com reviews',
            f'"{real_name}" site:foursquare.com reviews',
            f'"{real_name}" profile reviews',
            f'"{real_name}" restaurant reviewer',
            f'"{real_name}" food critic reviews'
        ]
        
        total_review_estimate = 0
        platform_counts = {}
        
        for query in platform_queries[:8]:  # Limit to 8 queries to avoid API exhaustion
            results = query_serper(query, num_results=3)
            
            if results:
                for result in results:
                    text_content = result if isinstance(result, str) else f"{result.get('title', '')} {result.get('snippet', '')}"
                    
                    # Enhanced review count patterns for cross-platform detection
                    import re
                    review_patterns = [
                        r'(\d+)\s+reviews?',                    # "48 reviews"
                        r'reviewed\s+(\d+)',                   # "reviewed 32"
                        r'(\d+)\s+restaurant\s+reviews?',      # "15 restaurant reviews"
                        r'(\d+)\s+food\s+reviews?',            # "22 food reviews"
                        r'has\s+written\s+(\d+)',              # "has written 48"
                        r'(\d+)\s+contributions?',             # "31 contributions"
                        r'(\d+)\s+helpful\s+votes?',           # "127 helpful votes"
                        r'(\d+)\s+photos?',                    # "15 photos" (indicates active reviewer)
                        r'-\s+(\d+)\s+Reviews\s+-',            # "- 32 Reviews -" (Yelp Elite format)
                        r'review_count[=:](\d+)',              # URL parameters
                        r'(\d+)\s+check-ins?',                 # Foursquare check-ins
                        r'(\d+)\s+tips?',                      # Foursquare tips
                        r'(\d+)\s+places?\s+reviewed',         # "12 places reviewed"
                        r'(\d+)\s+experiences?\s+shared',      # TripAdvisor format
                        r'Level\s+\d+\s+Local\s+Guide.*?(\d+)\s+reviews', # Google Local Guide
                    ]
                    
                    for pattern in review_patterns:
                        matches = re.findall(pattern, text_content, re.IGNORECASE)
                        if matches:
                            try:
                                review_count = int(matches[0])
                                # Only count significant review volumes (filter out noise)
                                if review_count >= 3 and review_count <= 1000:  # Reasonable bounds
                                    if review_count > total_review_estimate:
                                        total_review_estimate = review_count
                                        
                                    # Track per-platform for detailed analysis
                                    platform = "Unknown"
                                    if "yelp.com" in query:
                                        platform = "Yelp"
                                    elif "tripadvisor.com" in query:
                                        platform = "TripAdvisor"
                                    elif "google.com" in query:
                                        platform = "Google"
                                    elif "trustpilot.com" in query:
                                        platform = "TrustPilot"
                                    elif "facebook.com" in query:
                                        platform = "Facebook"
                                    elif "zomato.com" in query:
                                        platform = "Zomato"
                                    
                                    if platform not in platform_counts:
                                        platform_counts[platform] = review_count
                                    else:
                                        platform_counts[platform] = max(platform_counts[platform], review_count)
                                        
                                    print(f"üìä {platform} review volume detected: {review_count} reviews for {real_name}")
                                    
                            except (ValueError, IndexError):
                                continue
        
        # Calculate aggregate review volume across platforms
        if platform_counts:
            # Use highest single platform count, but boost for multi-platform presence
            max_single_platform = max(platform_counts.values())
            platform_count = len(platform_counts)
            
            # Multi-platform boost: +50% for 2 platforms, +100% for 3+
            if platform_count >= 3:
                total_review_estimate = int(max_single_platform * 2.0)
                print(f"üåê Multi-platform critic detected: {platform_count} platforms, boosted estimate: {total_review_estimate}")
            elif platform_count == 2:
                total_review_estimate = int(max_single_platform * 1.5)
                print(f"üåê Cross-platform reviewer: {platform_count} platforms, boosted estimate: {total_review_estimate}")
            else:
                total_review_estimate = max_single_platform
                
            print(f"üìà Platform breakdown: {platform_counts}")
        
        return total_review_estimate
        
    except Exception as e:
        print(f"‚ö†Ô∏è Cross-platform review volume estimation error for {real_name}: {e}")
        return 0

def estimate_review_volume(real_name):
    """
    Cross-platform review volume estimation for critic detection
    Searches across Yelp, TripAdvisor, Google, TrustPilot, Facebook, Zomato, etc.
    Returns estimated total review count across all platforms
    """
    if not real_name or real_name == "Unknown":
        return 0
    
    try:
        # Cross-platform search queries
        platform_queries = [
            f'"{real_name}" site:yelp.com reviews',
            f'"{real_name}" site:tripadvisor.com reviews', 
            f'"{real_name}" site:google.com reviews',
            f'"{real_name}" site:trustpilot.com reviews',
            f'"{real_name}" site:facebook.com reviews',
            f'"{real_name}" site:zomato.com reviews',
            f'"{real_name}" site:opentable.com reviews',
            f'"{real_name}" site:foursquare.com reviews',
            f'"{real_name}" profile reviews',
            f'"{real_name}" restaurant reviewer',
            f'"{real_name}" food critic reviews'
        ]
        
        total_review_estimate = 0
        platform_counts = {}
        
        for query in platform_queries[:8]:  # Limit to 8 queries to avoid API exhaustion
            results = query_serper(query, num_results=3)
            
            if results:
                for result in results:
                    text_content = result if isinstance(result, str) else f"{result.get('title', '')} {result.get('snippet', '')}"
                    
                    # Enhanced review count patterns for cross-platform detection
                    import re
                    review_patterns = [
                        r'(\d+)\s+reviews?',                    # "48 reviews"
                        r'reviewed\s+(\d+)',                   # "reviewed 32"
                        r'(\d+)\s+restaurant\s+reviews?',      # "15 restaurant reviews"
                        r'(\d+)\s+food\s+reviews?',            # "22 food reviews"
                        r'has\s+written\s+(\d+)',              # "has written 48"
                        r'(\d+)\s+contributions?',             # "31 contributions"
                        r'(\d+)\s+helpful\s+votes?',           # "127 helpful votes"
                        r'(\d+)\s+photos?',                    # "15 photos" (indicates active reviewer)
                        r'-\s+(\d+)\s+Reviews\s+-',            # "- 32 Reviews -" (Yelp Elite format)
                        r'review_count[=:](\d+)',              # URL parameters
                        r'(\d+)\s+check-ins?',                 # Foursquare check-ins
                        r'(\d+)\s+tips?',                      # Foursquare tips
                        r'(\d+)\s+places?\s+reviewed',         # "12 places reviewed"
                        r'(\d+)\s+experiences?\s+shared',      # TripAdvisor format
                        r'Level\s+\d+\s+Local\s+Guide.*?(\d+)\s+reviews', # Google Local Guide
                    ]
                    
                    for pattern in review_patterns:
                        matches = re.findall(pattern, text_content, re.IGNORECASE)
                        if matches:
                            try:
                                review_count = int(matches[0])
                                # Only count significant review volumes (filter out noise)
                                if review_count >= 3 and review_count <= 1000:  # Reasonable bounds
                                    if review_count > total_review_estimate:
                                        total_review_estimate = review_count
                                        
                                    # Track per-platform for detailed analysis
                                    platform = "Unknown"
                                    if "yelp.com" in query:
                                        platform = "Yelp"
                                    elif "tripadvisor.com" in query:
                                        platform = "TripAdvisor"
                                    elif "google.com" in query:
                                        platform = "Google"
                                    elif "trustpilot.com" in query:
                                        platform = "TrustPilot"
                                    elif "facebook.com" in query:
                                        platform = "Facebook"
                                    elif "zomato.com" in query:
                                        platform = "Zomato"
                                    
                                    if platform not in platform_counts:
                                        platform_counts[platform] = review_count
                                    else:
                                        platform_counts[platform] = max(platform_counts[platform], review_count)
                                        
                                    print(f"üìä {platform} review volume detected: {review_count} reviews for {real_name}")
                                    
                            except (ValueError, IndexError):
                                continue
        
        # Calculate aggregate review volume across platforms
        if platform_counts:
            # Use highest single platform count, but boost for multi-platform presence
            max_single_platform = max(platform_counts.values())
            platform_count = len(platform_counts)
            
            # Multi-platform boost: +50% for 2 platforms, +100% for 3+
            if platform_count >= 3:
                total_review_estimate = int(max_single_platform * 2.0)
                print(f"üåê Multi-platform critic detected: {platform_count} platforms, boosted estimate: {total_review_estimate}")
            elif platform_count == 2:
                total_review_estimate = int(max_single_platform * 1.5)
                print(f"üåê Cross-platform reviewer: {platform_count} platforms, boosted estimate: {total_review_estimate}")
            else:
                total_review_estimate = max_single_platform
                
            print(f"üìà Platform breakdown: {platform_counts}")
        
        return total_review_estimate
        
    except Exception as e:
        print(f"‚ö†Ô∏è Cross-platform review volume estimation error for {real_name}: {e}")
        return 0

def estimate_yelp_review_volume(real_name):
    """
    Legacy wrapper for backwards compatibility
    Now uses the cross-platform estimate_review_volume function
    """
    return estimate_review_volume(real_name)

def generate_platform_queries(name, location, phrases=[]):
    """Generate platform-specific search queries for MRI scanning"""
    base_terms = [f'"{name}"']
    if location:
        base_terms.append(f'"{location}"')
    for p in phrases:
        base_terms.append(f'"{p}"')

    combined = " ".join(base_terms)

    platforms = [
        "site:whitepages.com",
        "site:fastpeoplesearch.com",
        "site:truepeoplesearch.com",
        "site:spokeo.com",
        "site:radaris.com",
        "site:reddit.com",
        "site:trustpilot.com",
        "site:truthsocial.com",
        "site:facebook.com",
        "site:linkedin.com",
        "site:twitter.com",
        "site:instagram.com",
        "site:youtube.com",
        "site:yelp.com",
        "site:tripadvisor.com",
        "site:google.com"
    ]

    result = [f"{site} {combined}" for site in platforms]
    print(f"‚úÖ Generated {len(platforms)} platform queries for '{name}' in '{location}'")
    return result

def extract_identity_clues(results, handle):
    """Extract potential identity clues from search results"""
    clues = set()
    handle_lower = handle.lower()

    for result in results:
        if isinstance(result, dict):
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
        else:
            title = ""
            snippet = str(result).lower()

        # Look for full names
        name_patterns = [
            r'\b' + re.escape(handle_lower) + r'\s+([a-z]+)\b',
            r'\b([a-z]+)\s+' + re.escape(handle_lower) + r'\b'
        ]

        for pattern in name_patterns:
            matches = re.findall(pattern, title + ' ' + snippet)
            for match in matches:
                if len(match) > 2:
                    clues.add(f"{handle} {match.title()}")

        # Look for contact info
        email_pattern = r'\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b'
        phone_pattern = r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'

        emails = re.findall(email_pattern, snippet)
        phones = re.findall(phone_pattern, snippet)

        for email in emails:
            clues.add(f"email:{email}")
        for phone in phones:
            clues.add(f"phone:{phone}")

    return clues

def purge_duplicate_aliases():
    """
    Clean up alias cache to remove duplicate aliases with different punctuation.
    Normalizes all aliases to prevent Seth D. vs Seth D inconsistencies.
    """
    try:
        with open("alias_cache.json", "r") as f:
            data = json.load(f)
    except FileNotFoundError:
        print("No alias cache found to clean.")
        return

    print(f"üßπ Cleaning alias cache: {len(data)} entries before normalization")
    
    normalized = {}
    conflicts = []
    
    for original_alias, identity in data.items():
        normalized_key = normalize_alias(original_alias)
        
        if normalized_key in normalized:
            # Conflict detected - prefer the identity with more detail
            existing_identity = normalized[normalized_key]
            if len(identity.split()) > len(existing_identity.split()):
                conflicts.append(f"Replaced '{existing_identity}' with '{identity}' for key '{normalized_key}'")
                normalized[normalized_key] = identity
            else:
                conflicts.append(f"Kept '{existing_identity}' over '{identity}' for key '{normalized_key}'")
        else:
            normalized[normalized_key] = identity

    # Save cleaned cache
    with open("alias_cache.json", "w") as f:
        json.dump(normalized, f, indent=2)
    
    print(f"‚úÖ Cache cleaned: {len(normalized)} unique normalized entries")
    if conflicts:
        print(f"‚ö†Ô∏è Resolved {len(conflicts)} conflicts:")
        for conflict in conflicts[:5]:  # Show first 5 conflicts
            print(f"   ‚Ä¢ {conflict}")
    
    return len(data) - len(normalized)  # Return number of duplicates removed

def scrape_contact_info(url, verbose=False):
    """
    Enhanced contact info scraping using Puppeteer + Cheerio via Render endpoint
    Returns structured contact information extracted from the page
    """
    try:
        scraper_url = "https://controll-scraper.onrender.com/scrape"
        response = requests.post(scraper_url, json={"url": url}, timeout=10)

        if response.status_code == 200:
            html = response.json().get("html", "")
            if verbose:
                print(f"üß† Scraped HTML from {url}:\n", html[:1000])

            # Extract emails and phones
            emails = re.findall(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", html)
            phones = re.findall(r"\(?\b[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}\b", html)

            # Detect review platform URLs
            review_platforms = []
            known_review_sites = [
                "yelp.com", "tripadvisor.com", "zomato.com", "trustpilot.com",
                "opentable.com", "booking.com", "glassdoor.com"
            ]
            for site in known_review_sites:
                if site in html:
                    review_platforms.append(site)

            # Detect social media links
            social_links = []
            known_social_sites = [
                "facebook.com", "linkedin.com", "twitter.com", "instagram.com",
                "tiktok.com", "threads.net", "youtube.com"
            ]
            for site in known_social_sites:
                if site in html:
                    social_links.append(site)

            # Clean and filter results
            clean_emails = []
            for email in emails:
                email = email.lower().strip()
                if "@" in email and "." in email.split("@")[1]:
                    if not filter_junk_identity(email=email):
                        clean_emails.append(email)
                    elif verbose:
                        print(f"üö´ Filtered junk email: {email}")

            clean_phones = []
            for phone in phones:
                clean_phone = re.sub(r'[^\d]', '', str(phone))
                if len(clean_phone) >= 10:
                    if not filter_junk_identity(phone=clean_phone):
                        clean_phones.append(clean_phone)
                    elif verbose:
                        print(f"üö´ Filtered junk phone: {clean_phone}")

            result = {
                "emails": list(set(clean_emails)),
                "phones": list(set(clean_phones)),
                "review_platforms": review_platforms,
                "social_links": social_links,
                "html_snippet": html[:1000]  # optional for debugging
            }
            
            if verbose:
                print(f"‚úÖ Contact extraction complete: {len(result['emails'])} emails, {len(result['phones'])} phones")
                if result["emails"]:
                    print(f"   üìß Emails: {result['emails']}")
                if result["phones"]:
                    print(f"   üìû Phones: {result['phones']}")
                if review_platforms:
                    print(f"   üìù Review platforms: {review_platforms}")
                if social_links:
                    print(f"   üì± Social links: {social_links}")
            
            return result
        else:
            return {"error": f"Failed to scrape: {response.status_code}"}
    except Exception as e:
        return {"error": str(e)}

def check_phone_penetration(phone_number):
    """Search for phone number across known review/social platforms using query_serper()."""
    platforms = [
        "yelp.com", "tripadvisor.com", "reddit.com", "facebook.com", "glassdoor.com",
        "nextdoor.com", "trustpilot.com", "blogspot.com", "medium.com", "quora.com"
    ]
    matches = []
    for site in platforms:
        try:
            results = query_serper(f'"{phone_number}" site:{site}')
            if results and len(results) > 0:
                matches.append(site)
        except Exception as e:
            print(f"‚ùå Error checking {site}: {e}")

    if matches:
        # PATCH 2: Aggressive platform-based risk boost
        risk_boost = boost_risk_by_platform_penetration(matches)
        print(f"\nüì° Cross-Platform Penetration: {len(matches)} matches (Risk +{risk_boost})")
        for match in matches:
            print(f"- Found on {match}")

    return matches

# Code analysis: This code applies a fix to enhance Yelp review count detection and critic behavior identification during profile extraction.
